{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_classes = [0,85, 170]\n",
    "class_names = [\"Background\",\"Pupil\",\"Iris\"]\n",
    "\n",
    "\n",
    "class_map = dict(zip(valid_classes, range(len(valid_classes))))\n",
    "n_classes=len(valid_classes)\n",
    "\n",
    "\n",
    "colors = [ [  0,   0,   0],[0,255,0],[0,0,255]]\n",
    "label_colours = dict(zip(range(n_classes), colors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.measure import label,regionprops,find_contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"/home/nipun/Documents/Uni_Malta/Datasets/Datasets/Miche/Miche_Dataset/train_img/009_GT2_IN_F_LI_01_2.jpg\"\n",
    "mask_path = \"/home/nipun/Documents/Uni_Malta/Datasets/Datasets/Miche/Miche_Dataset/train_masks/009_GT2_IN_F_LI_01_2.bmp\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(img_path,cv2.IMREAD_COLOR) # Read Image as BGR\n",
    "\n",
    "print(img.shape)\n",
    "# h,w = img.shape[:2]\n",
    "h,w = 400,400\n",
    "\n",
    "img = cv2.resize(img,(w,h))\n",
    "\n",
    "\n",
    "plt.imshow(img[:,:,::-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = cv2.imread(mask_path)\n",
    "\n",
    "mask = cv2.cvtColor(cv2.resize(mask,(w,h)),cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "print(mask.shape)\n",
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = cv2.medianBlur(mask, 5)\n",
    "        \n",
    "edge_detected_image = cv2.Canny(median, 12, 234)\n",
    "\n",
    "\n",
    "contours, hierarchy = cv2.findContours(edge_detected_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(edge_detected_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(contours)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_mask = np.zeros_like(img)\n",
    "\n",
    "\n",
    "print(bg_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_area = {}\n",
    "for contour in contours:\n",
    "    \n",
    "    approx = cv2.approxPolyDP(contour,0.01*cv2.arcLength(contour,True),True)\n",
    "    area = cv2.contourArea(contour)\n",
    "    # print(area)\n",
    "    if ((len(approx) > 8) & (len(approx) < 23) & (area > 30) ):\n",
    "        \n",
    "        \n",
    "        max_area[area] = contour\n",
    "        \n",
    "\n",
    "x = sorted(max_area,key = lambda x:x)\n",
    "print(x)\n",
    "max_contour = max_area[x[-1]]\n",
    "min_contour = max_area[x[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.drawContours(bg_mask, [max_contour],  0, (0,255,0), -1)\n",
    "\n",
    "cv2.drawContours(bg_mask,[min_contour],0,(0,0,255),-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(bg_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = [[0,0,0],[0,255,0],[0,0,255]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# area = cv2.contourArea(min_contour)\n",
    "# print(area)\n",
    "\n",
    "print(np.unique(bg_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_mask = []\n",
    "for i,color in enumerate(color_map):\n",
    "    cmap = np.all(np.equal(bg_mask,color),axis=-1)\n",
    "    output_mask.append(cmap)\n",
    "output_mask = np.stack(output_mask,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscale_mask = np.argmax(output_mask, axis=-1)\n",
    "grayscale_mask = (grayscale_mask / len(color_map)) * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(grayscale_mask)\n",
    "cv2.imwrite(\"Inspection_1.png\",grayscale_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(img[:,:,::-1])\n",
    "plt.imshow(grayscale_mask,alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_image = cv2.resize(img,(64,64))\n",
    "\n",
    "resized_mask = cv2.resize(grayscale_mask,(256,256))\n",
    "\n",
    "\n",
    "resized_image = cv2.resize(resized_image,(256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask Changes\n",
    "# if masks are resized into image sizes then shape of the mask won't change even mask and image have different image dimensions.\n",
    "# if masks and images are resized into fixed image sizes ,mask shape won't change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(resized_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"Inspection_2.png\",resized_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_max = cv2.approxPolyDP(max_contour,0.001*cv2.arcLength(max_contour,True),True)\n",
    "approx_min =  cv2.approxPolyDP(min_contour,0.001*cv2.arcLength(min_contour,True),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv2.contourArea(max_contour))\n",
    "print(cv2.contourArea(approx_max))\n",
    "print(cv2.contourArea(min_contour))\n",
    "print(cv2.contourArea(approx_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_up_1 = np.zeros_like(img)\n",
    "# print(bg_up_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.drawContours(bg_up_1 , [approx_max],  0, (0,255,0), cv2.FILLED)\n",
    "\n",
    "cv2.drawContours(bg_up_1 ,[approx_min],0,(0,0,255),cv2.FILLED) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(bg_up_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(img[:,:,::-1])\n",
    "plt.imshow(bg_up_1,alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"Inspection_3.png\",bg_up_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"Inspection_4.png\",cv2.resize(bg_up_1,(256,256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_1 = []\n",
    "for coords in approx_max:\n",
    "    # print(coords[0])\n",
    "    poly_1.append(coords[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_2 = []\n",
    "for coords in approx_min:\n",
    "    # print(coords[0])\n",
    "    poly_2.append(coords[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon1 = Polygon(poly_1)\n",
    "polygon2 = Polygon(poly_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = polygon1.difference(polygon2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_max.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_max[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect_part = list(difference.exterior.coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_up = []\n",
    "for coords in intersect_part:\n",
    "    cont_up.append([coords])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_up = np.array(cont_up).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_up.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect_part[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bg_up_2 = np.zeros_like(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.drawContours(bg_up_2 , [cont_up],  0, (0,255,0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(bg_up_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image,ImageFilter\n",
    "\n",
    "mask_path = \"/home/nipun/Documents/Uni_Malta/Datasets/Datasets/Miche/Miche_Dataset/train_masks/003_GS4_IN_R_RI_01_1.bmp\"\n",
    "\n",
    "img = Image.open(mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im_5 = im_1.filter(ImageFilter.EDGE_ENHANCE) \n",
    "im_6 = img.filter(ImageFilter.EDGE_ENHANCE_MORE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_6.save(\"EdgeEnhanced_Image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = cv2.imread(\"/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Experiments/utils/Masks_with_256/075_GT2_OU_F_LI_01_1.png\",0)\n",
    "img = cv2.imread(\"/home/nipun/Documents/Uni_Malta/Datasets/Datasets/Miche/Miche_Dataset/train_img/075_GT2_OU_F_LI_01_1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.resize(img,(256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(img[:,:,::-1])\n",
    "\n",
    "mask = decode_segmap(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(img[:,:,::-1])\n",
    "plt.imshow(mask,alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img[:,:,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colors = [ [  0,   0,   0],[0,255,0],[0,0,255]]\n",
    "label_colours = dict(zip(range(n_classes), colors))\n",
    "\n",
    "valid_classes = [0,85, 170]\n",
    "class_names = [\"Background\",\"Pupil\",\"Iris\"]\n",
    "\n",
    "\n",
    "class_map = dict(zip(valid_classes, range(len(valid_classes))))\n",
    "n_classes=len(valid_classes)\n",
    "\n",
    "\n",
    "def decode_segmap(temp):\n",
    "    #convert gray scale to color\n",
    "    # temp=temp.numpy()\n",
    "    r = temp.copy()\n",
    "    g = temp.copy()\n",
    "    b = temp.copy()\n",
    "    for l in range(0, n_classes):\n",
    "        r[temp == l] = label_colours[l][0]\n",
    "        g[temp == l] = label_colours[l][1]\n",
    "        b[temp == l] = label_colours[l][2]\n",
    "\n",
    "    rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n",
    "    rgb[:, :, 0] = r / 255.0\n",
    "    rgb[:, :, 1] = g / 255.0\n",
    "    rgb[:, :, 2] = b / 255.0\n",
    "    return rgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = cv2.imread(\"/home/nipun/Documents/Uni_Malta/Datasets/Datasets/Miche/Miche_Dataset/train_masks/075_GT2_OU_F_LI_01_1.bmp\")\n",
    "img = cv2.imread(\"/home/nipun/Documents/Uni_Malta/Datasets/Datasets/Miche/Miche_Dataset/train_img/075_GT2_OU_F_LI_01_1.jpg\")\n",
    "\n",
    "\n",
    "print(img.shape)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(img[:,:,::-1])\n",
    "plt.imshow(mask,alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = cv2.medianBlur(mask, 5)\n",
    "        \n",
    "edge_detected_image = cv2.Canny(median, 12, 234)\n",
    "\n",
    "\n",
    "contours, hierarchy = cv2.findContours(edge_detected_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_area = {}\n",
    "for contour in contours:\n",
    "    \n",
    "    approx = cv2.approxPolyDP(contour,0.01*cv2.arcLength(contour,True),True)\n",
    "    area = cv2.contourArea(contour)\n",
    "    # print(area)\n",
    "    if ((len(approx) > 8) & (len(approx) < 23) & (area > 30) ):\n",
    "        \n",
    "        \n",
    "        max_area[area] = contour\n",
    "        \n",
    "\n",
    "x = sorted(max_area,key = lambda x:x)\n",
    "print(x)\n",
    "max_contour = max_area[x[-1]]\n",
    "min_contour = max_area[x[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_max = cv2.approxPolyDP(max_contour,0.001*cv2.arcLength(max_contour,True),True)\n",
    "approx_min =  cv2.approxPolyDP(min_contour,0.001*cv2.arcLength(min_contour,True),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = np.zeros_like(img)\n",
    "\n",
    "print(m1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.drawContours(m1 , [approx_max],  0, (0,255,0), cv2.FILLED)\n",
    "\n",
    "cv2.drawContours(m1 ,[approx_min],0,(0,0,255),cv2.FILLED) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(img[:,:,::-1])\n",
    "plt.imshow(m1,alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"Inspection_4.png\",m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mediapipe\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = '/home/nipun/Pictures/My_New_Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "saved_dir = \"./eyes_mine\"\n",
    "\n",
    "\n",
    "if not os.path.exists(saved_dir):\n",
    "    os.makedirs(saved_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpArrayToNumpy(landmark_array, img):\n",
    "\n",
    "    shape_arr = []\n",
    "\n",
    "    for landmark in landmark_array.landmark:\n",
    "        x = landmark.x\n",
    "        y = landmark.y\n",
    "\n",
    "        relative_x = int(x * img.shape[1])\n",
    "        relative_y = int(y * img.shape[0])\n",
    "\n",
    "        shape_arr.append([relative_x, relative_y])\n",
    "\n",
    "    return np.array(shape_arr)\n",
    "\n",
    "\n",
    "def cropped_image(img, shape_array, padded_amt=30):\n",
    "    \"\"\"Cropped eye region \n",
    "\n",
    "    Args:\n",
    "        img (__numpy__): _Original Image_\n",
    "        shape_array (_numpy_): _FaceLandMark locations_\n",
    "        padded_amt (int, optional): _padding size_. Defaults to 15.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Leye = {\"top_left\": shape_array[70], \"bottom_right\": shape_array[133]}\n",
    "\n",
    "    Reye = {\"top_left\": shape_array[285],\n",
    "            \"bottom_right\": shape_array[446]}\n",
    "\n",
    "    left_eye = img[Leye[\"top_left\"][1]:Leye[\"bottom_right\"][1] +\n",
    "                   15\n",
    "                   , Leye[\"top_left\"][0]:Leye[\"bottom_right\"][0]]\n",
    "\n",
    "    right_eye = img[Reye[\"top_left\"][1]:Reye[\"bottom_right\"][1] +\n",
    "                    30, Reye[\"top_left\"][0]-5:Reye[\"bottom_right\"][0]]\n",
    "    \n",
    "    \n",
    "    \n",
    "    Reye['top_left'][0] = Reye['top_left'][0] - 5\n",
    "\n",
    "    return left_eye, right_eye, Leye, Reye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mediapipe.solutions.face_mesh\n",
    "\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count = 0\n",
    "visualize = True\n",
    "data_array =[]\n",
    "\n",
    "\n",
    "for img_path in os.scandir(image_dir):\n",
    "\n",
    "    \n",
    "    img = cv2.imread(img_path.path)\n",
    "\n",
    "    \n",
    "    \n",
    "    results = face_mesh.process(img)\n",
    "    \n",
    "    if results.multi_face_landmarks is not None:\n",
    "        image_count +=1\n",
    "        \n",
    "        img_name = f'I2head_{image_count}'\n",
    "        \n",
    "        landmarks = results.multi_face_landmarks[0]\n",
    "\n",
    "        shape_arr = mpArrayToNumpy(landmarks, img)\n",
    "\n",
    "        left_eye, right_eye, Leye, Reye = cropped_image(\n",
    "            img, shape_arr)\n",
    "        \n",
    "\n",
    "\n",
    "        cv2.imwrite(os.path.join(\n",
    "            saved_dir, f\"{img_name}_left.png\"), left_eye)\n",
    "\n",
    "        cv2.imwrite(os.path.join(\n",
    "            saved_dir, f\"{img_name}_right.png\"), right_eye)\n",
    "\n",
    "        if visualize:\n",
    "\n",
    "            \n",
    "\n",
    "            fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "            axs[0].set_title(\"Left Eye\")\n",
    "            axs[1].set_title(\"Right Eye\")\n",
    "\n",
    "            axs[0].axis(\"off\")\n",
    "            axs[1].axis(\"off\")\n",
    "\n",
    "            axs[0].imshow(left_eye[:, :, ::-1])\n",
    "            axs[1].imshow(right_eye[:, :, ::-1])\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close('all')\n",
    "\n",
    "    else:\n",
    "        print(\n",
    "            f\"MediaPipe was failed to detect the faces on the image name {img_name}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yolo Cropping Eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import albumentations as A\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import time \n",
    "device =torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "train_images = \"/home/nipun/Documents/Uni_Malta/Datasets/Datasets/Miche/train_img\"\n",
    "train_masks  = \"/home/nipun/Documents/Uni_Malta/Datasets/Datasets/Miche/train_masks\"\n",
    "\n",
    "\n",
    "val_images = \"/home/nipun/Documents/Uni_Malta/Datasets/Datasets/Miche/val_img\"\n",
    "\n",
    "val_masks =  \"/home/nipun/Documents/Uni_Malta/Datasets/Datasets/Miche/val_masks\" \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_x = sorted(\n",
    "        glob(f\"{train_images}/*\"))\n",
    "train_y = sorted(\n",
    "        \n",
    "        glob(f\"{train_masks}/*\"))\n",
    "valid_x = sorted(\n",
    "        glob(f\"{val_images}/*\"))\n",
    "valid_y = sorted(\n",
    "        glob(f\"{val_masks }/*\"))\n",
    "\n",
    "\n",
    "yolo_model_path = \"/home/nipun/Music/yolov5/runs/train/exp8/weights/best.pt\"\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path=yolo_model_path ,force_reload=True)\n",
    "\n",
    "# Filter BBOX Based on Confidence\n",
    "model.conf = 0.40\n",
    "\n",
    "\n",
    "saved_img_location = \"/home/nipun/Documents/Uni_Malta/Datasets/Datasets/Miche/YoloCroppedEyes/images\"\n",
    "saved_mask_location = \"/home/nipun/Documents/Uni_Malta/Datasets/Datasets/Miche/YoloCroppedEyes/masks\"\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(saved_img_location ):\n",
    "    os.makedirs(saved_img_location)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "if not os.path.exists(saved_mask_location):\n",
    "    os.makedirs(saved_mask_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_contours(image):\n",
    "    gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "    median = cv2.medianBlur(gray, 5)\n",
    "        \n",
    "    edge_detected_image = cv2.Canny(median, 0, 200)\n",
    "    \n",
    "\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(edge_detected_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    return contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "padded_amt = 15\n",
    "\n",
    "pad_top_left_y1 = 20\n",
    "pad_bottom_right_x2 = 7\n",
    "RESIZE_AMT = 256\n",
    "\n",
    "\n",
    "rows = len(valid_x)\n",
    "\n",
    "\n",
    "\n",
    "for z, (img_path, mask_path) in enumerate(zip(train_x, train_y)):\n",
    "\n",
    "    \n",
    "    image = cv2.imread(img_path)\n",
    "\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    mask =  cv2.imread(mask_path)\n",
    "    \n",
    "    \n",
    "    contour_img = np.zeros_like(image)\n",
    "    \n",
    "    \n",
    "    contours = find_contours(mask)\n",
    "    \n",
    "    max_area = {}\n",
    "    for contour in contours:\n",
    "        \n",
    "        approx = cv2.approxPolyDP(contour,0.01*cv2.arcLength(contour,True),True)\n",
    "        area = cv2.contourArea(contour)\n",
    "        # print(area)\n",
    "        if ((len(approx) > 8) & (len(approx) < 23) & (area > 30) ):\n",
    "            \n",
    "            \n",
    "            max_area[area] = contour\n",
    "            \n",
    "    \n",
    "    x = sorted(max_area,key = lambda x:x)\n",
    "\n",
    "    max_contour = max_area[x[-1]]\n",
    "    min_contour = max_area[x[1]]\n",
    "\n",
    "    cv2.drawContours(contour_img ,[max_contour],0,(0,0,255),-1)\n",
    "\n",
    "    cv2.drawContours(contour_img ,[min_contour],0,(0,255,0),-1)\n",
    "    \n",
    "\n",
    "\n",
    "    results = model(image[:, :, ::-1], size=640)\n",
    "\n",
    "    df = results.pandas().xyxy[0]\n",
    "\n",
    "    # Get the all BBOX related to Iris Class which is zero\n",
    "    df = df[df[\"class\"] == 0]\n",
    "\n",
    "    df = df[df['confidence'] == df['confidence'].max()]\n",
    "\n",
    "    \n",
    "\n",
    "    for (i, row) in df.iterrows():\n",
    "\n",
    "        x1 = round(row[\"xmin\"])\n",
    "        y1 = round(row[\"ymin\"])\n",
    "        x2 = round(row[\"xmax\"])\n",
    "        y2 = round(row[\"ymax\"])\n",
    "\n",
    "   \n",
    "    copy_img = image.copy()\n",
    "\n",
    "    if y1 < pad_top_left_y1:\n",
    "        pad_top_left_y1 = y1\n",
    "\n",
    "    elif x1 < padded_amt:\n",
    "        padded_amt = x1\n",
    "    cropped_img = copy_img[y1-pad_top_left_y1:y2 +\n",
    "                                padded_amt, x1-padded_amt:x2+pad_bottom_right_x2]\n",
    "\n",
    "    cropped_mask = contour_img[y1-pad_top_left_y1:y2 +\n",
    "                                padded_amt, x1-padded_amt:x2+pad_bottom_right_x2]\n",
    "    cropped_mask = np.where(cropped_mask > 0, 255, 0)\n",
    "\n",
    "    image_name = img_path.split(\"/\")[-1]\n",
    "    mask_name = mask_path.split(\"/\")[-1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    draw_img = cropped_img.copy()\n",
    "    \n",
    "    gt_contours  = find_contours(cropped_mask.astype(np.uint8))\n",
    "    \n",
    "   \n",
    "    \n",
    "    for gt_cnt in gt_contours:\n",
    "            cv2.drawContours(draw_img, [gt_cnt],  -1, (0,255,0), 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.imshow(draw_img[:,:,::-1])\n",
    "    plt.show()\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"/home/nipun/Documents/Uni_Malta/Datasets/Datasets/Miche/train_img/011_GT2_IN_F_LI_01_1.jpg\"\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "mask_path = \"/home/nipun/Documents/Uni_Malta/Datasets/Datasets/Miche/train_masks/011_GT2_IN_F_LI_01_1.bmp\"\n",
    "mask = cv2.imread(mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(img[:,:,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mask[:,:,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contours = find_contours(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gt_cnt in contours:\n",
    "            cv2.drawContours(img, [gt_cnt],  -1, (0,255,0), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(img[:,:,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_img = np.zeros((img.shape[0],img.shape[1]))\n",
    "\n",
    "\n",
    "print(contour_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "contours = np.array( [ [50,50], [50,150], [150, 150], [150,50] ] )\n",
    "\n",
    "contours1 = np.array( [ [50,50], [50,150], [180, 150], [190,50] ] )\n",
    "img = np.zeros( (200,200) ) # create a single channel 200x200 pixel black image \n",
    "cv2.fillPoly(img, pts =[contours], color=1)\n",
    "cv2.fillPoly(img, pts =[contours1], color=2)\n",
    "\n",
    "\n",
    "plt.imshow(img)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contours = find_contours(mask)\n",
    "big_contour = max(contours, key=cv2.contourArea)\n",
    "peri = cv2.arcLength(big_contour, True)\n",
    "big_contour = cv2.approxPolyDP(big_contour, 0.001 * peri, True)\n",
    "cv2.drawContours(contour_img ,[big_contour], -1, (255,255,255), thickness=cv2.FILLED)\n",
    "\n",
    "plt.imshow(contour_img )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_contour = min(contours, key=cv2.contourArea)\n",
    "peri_min = cv2.arcLength(min_contour, True)\n",
    "min_contour = cv2.approxPolyDP(min_contour , 0.001 * peri_min, True)\n",
    "\n",
    "cv2.drawContours(contour_img ,[min_contour], -1, (255,0,0), thickness=cv2.FILLED)\n",
    "plt.imshow(contour_img )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LuminEye",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0a565227b7b463d99e303b82d66a46fc21535ffbac57e09021a6b5954f09e13"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
