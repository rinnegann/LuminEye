{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from albumentations import Compose, Normalize, RandomCrop, HorizontalFlip, ShiftScaleRotate, HueSaturationValue\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "from torchmetrics import AUROC\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import albumentations as A\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "from sklearn.metrics import f1_score,accuracy_score,recall_score,roc_auc_score,roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/abdelrahmanmuhsen/animal-body-parts-segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerModel, SegformerConfig\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import requests\n",
    "\n",
    "from transformers import SegformerPreTrainedModel, SegformerForSemanticSegmentation\n",
    "\n",
    "from transformers import SegformerModel, SegformerConfig\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = \"/home/nipun/Documents/Uni_Malta/Datasets/Datasets/Miche/MICHE_MULTICLASS/Dataset/train_img/\"\n",
    "train_masks  = \"/home/nipun/Documents/Uni_Malta/Datasets/Datasets/Miche/MICHE_MULTICLASS/Dataset/train_masks/\"\n",
    "\n",
    "\n",
    "val_images = \"/home/nipun/Documents/Uni_Malta/Datasets/Datasets/Miche/MICHE_MULTICLASS/Dataset/val_img\"\n",
    "\n",
    "val_masks =  \"/home/nipun/Documents/Uni_Malta/Datasets/Datasets/Miche/MICHE_MULTICLASS/Dataset/val_masks/\" \n",
    "n_classes = 3\n",
    "batch_size = 1\n",
    "\n",
    "train_x = sorted(\n",
    "        glob(f\"{train_images}/*\"))\n",
    "train_y = sorted(\n",
    "        glob(f\"{train_masks}/*\"))\n",
    "valid_x = sorted(\n",
    "        glob(f\"{val_images}/*\"))\n",
    "valid_y = sorted(\n",
    "        glob(f\"{val_masks }/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes =3\n",
    "colors = [ [  0,   0,   0],[0,255,0],[0,0,255]]\n",
    "label_colours = dict(zip(range(n_classes), colors))\n",
    "\n",
    "valid_classes = [0,85, 170]\n",
    "class_names = [\"Background\",\"Pupil\",\"Iris\"]\n",
    "\n",
    "\n",
    "class_map = dict(zip(valid_classes, range(len(valid_classes))))\n",
    "n_classes=len(valid_classes)\n",
    "\n",
    "id2label = {v:class_names[v] for k,v in class_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_resize = (224,224)\n",
    "class Iris(Dataset):\n",
    "    def __init__(self,images,masks,img_resize=img_resize,transform = None):\n",
    "        self.transform = transform\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.img_resize = img_resize\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self,index):\n",
    "        img = np.array(Image.open(self.images[index]).resize(self.img_resize)).astype(\"float32\")\n",
    "        \n",
    "        img = img/255.0\n",
    "        mask = np.array(Image.open(self.masks[index]).resize(self.img_resize))\n",
    "        \n",
    "        sample = {\"image\":img,\"mask\":mask}\n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nipun/mambaforge/envs/LuminEye/lib/python3.10/site-packages/transformers/models/segformer/feature_extraction_segformer.py:28: FutureWarning: The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n",
      "  warnings.warn(\n",
      "/home/nipun/mambaforge/envs/LuminEye/lib/python3.10/site-packages/transformers/models/segformer/image_processing_segformer.py:99: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/segformer-b4-finetuned-ade-512-512\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(sample):\n",
    "    data_transforms = Compose([                  \n",
    "                        ToTensorV2()\n",
    "                    ])\n",
    "    sample['image']= sample['image']\n",
    "    sample = data_transforms(**sample)\n",
    "    sample['image']=feature_extractor(sample['image'], return_tensors=\"pt\")\n",
    "    sample['image']=sample['image']['pixel_values']    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Iris(train_x,train_y,transform = transform)\n",
    "val_data  = Iris(valid_x,valid_y,transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_batch = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3, 512, 512])\n",
      "torch.Size([1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "ex_img_train = next(iter(train_batch))[\"image\"]\n",
    "ex_mask_train = next(iter(train_batch))[\"mask\"]\n",
    "\n",
    "print(ex_img_train.shape)\n",
    "print(ex_mask_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Experiments/SEGFormer/segformer_training.ipynb Cell 13\u001b[0m in \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Experiments/SEGFormer/segformer_training.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m15\u001b[39m,\u001b[39m15\u001b[39m))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Experiments/SEGFormer/segformer_training.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(ex_mask_train[\u001b[39m1\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Experiments/SEGFormer/segformer_training.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39munique(ex_mask_train))\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(ex_mask_train[1])\n",
    "print(np.unique(ex_mask_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSegFormer2(SegformerPreTrainedModel):\n",
    "    def __init__(self, config, in_channel, n_classes=3):\n",
    "        super().__init__(config)\n",
    "        self.segformer = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b4-finetuned-ade-512-512\", ignore_mismatched_sizes=True)\n",
    "        self.logits_outputs = nn.Conv2d(in_channel, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        # forward to get raw logits\n",
    "        pixel_values=torch.squeeze(pixel_values, dim=1)\n",
    "        outputs = self.segformer(pixel_values) # what's the number of output channels?\n",
    "        logits_outputs = self.logits_outputs(outputs[0])\n",
    "        # next, do whatever you want with the logits to compute loss\n",
    "        # upsample logits here; the predicted distance map is also needed to unsampled, right?\n",
    "        upsampled_logits = nn.functional.interpolate(logits_outputs, size=img_shape, mode=\"bilinear\", align_corners=False)\n",
    "        # calculate losses for logits and distance map respectively and sum the two losses\n",
    "        \n",
    "        return  upsampled_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerModel, SegformerConfig\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "configuration = SegformerConfig()\n",
    "model=CustomSegFormer2(configuration,in_channel=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "dice_loss=smp.losses.DiceLoss('multiclass')\n",
    "def iou(output, target):\n",
    "    output=F.softmax(output, dim=1)\n",
    "    output = torch.argmax(output, dim=1)\n",
    "    tp, fp, fn, tn = smp.metrics.get_stats(output, target, mode='multiclass',num_classes=3)\n",
    "    return smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, train_loader, val_loader, criterion, optimizer):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    val_iou = []; \n",
    "    train_iou = []; \n",
    "    min_loss = np.inf\n",
    "    max_val_iou_score=0\n",
    "    increase = 0 ; not_improve=0\n",
    "\n",
    "    model.to(device)\n",
    "    fit_time = time.time()\n",
    "    for e in range(epochs):\n",
    "        since = time.time()\n",
    "        running_loss = 0\n",
    "        iou_score = 0\n",
    "        accuracy = 0\n",
    "        #training loop\n",
    "        model.train()\n",
    "        for i, data in enumerate(train_loader):\n",
    "            #training phase\n",
    "            image_tiles = data['image']\n",
    "            mask_tiles = data['mask']\n",
    "\n",
    "\n",
    "            image = image_tiles.to(device)\n",
    "            mask = mask_tiles.type(torch.LongTensor).to(device)\n",
    "            #forward\n",
    "            output = model(image).to(device)\n",
    "            #loss = criterion(output, mask.type(torch.LongTensor).to(device)).requires_grad_()\n",
    "            loss =criterion(output, mask)\n",
    "            #evaluation metrics\n",
    "            iou_score += iou(output, mask)\n",
    "            #backward\n",
    "            loss.backward()\n",
    "            optimizer.step() #update weight          \n",
    "            optimizer.zero_grad() #reset gradient\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            val_iou_score = 0\n",
    "        #validation loop\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for i, data in enumerate(val_loader):\n",
    "                image_tiles = data['image']\n",
    "                mask_tiles = data['mask']\n",
    "\n",
    "                image = image_tiles.to(device); mask = mask_tiles.type(torch.LongTensor).to(device);\n",
    "                output = model(image)\n",
    "                #evaluation metrics\n",
    "                val_iou_score +=  iou(output, mask)\n",
    "                #loss\n",
    "                loss = criterion(output, mask)                                  \n",
    "                test_loss += loss.item()\n",
    "\n",
    "        #calculatio mean for each batch\n",
    "        train_losses.append(running_loss/len(train_loader))\n",
    "        test_losses.append(test_loss/len(val_loader))\n",
    "\n",
    "\n",
    "        if max_val_iou_score < (val_iou_score/len(val_loader)):\n",
    "            print('val_iou_increased.. {:.3f} >> {:.3f} '.format(max_val_iou_score, (val_iou_score/len(val_loader))))\n",
    "            max_val_iou_score =  (val_iou_score/len(val_loader))\n",
    "            increase += 1\n",
    "            not_improve=0\n",
    "            if increase % 5 == 0:\n",
    "                print('saving model...')\n",
    "                torch.save(model, 'segformer-{:.3f}.pt'.format(val_iou_score/len(val_loader)))\n",
    "\n",
    "        else :\n",
    "            not_improve += 1\n",
    "            if not_improve == 7:\n",
    "                print('Loss not decrease for 7 times, Stop Training')\n",
    "                break\n",
    "\n",
    "\n",
    "        #iou\n",
    "        val_iou.append(val_iou_score/len(val_loader))\n",
    "        train_iou.append(iou_score/len(train_loader))\n",
    "        print(\"Epoch:{}/{}..\".format(e+1, epochs),\n",
    "              \"Train Loss: {:.3f}..\".format(running_loss/len(train_loader)),\n",
    "              \"Val Loss: {:.3f}..\".format(test_loss/len(val_loader)),\n",
    "              \"Train mIoU:{:.3f}..\".format(iou_score/len(train_loader)),\n",
    "              \"Val mIoU: {:.3f}..\".format(val_iou_score/len(val_loader)),\n",
    "              \"Time: {:.2f}m\".format((time.time()-since)/60))\n",
    "\n",
    "    history = {'train_loss' : train_losses, 'val_loss': test_losses,\n",
    "               'train_miou' :train_iou, 'val_miou':val_iou,\n",
    "               }\n",
    "    print('Total time: {:.2f} m' .format((time.time()- fit_time)/60))\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 5.81 GiB total capacity; 677.46 MiB already allocated; 10.00 MiB free; 698.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Experiments/SEGFormer/segformer_training.ipynb Cell 18\u001b[0m in \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Experiments/SEGFormer/segformer_training.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m criterion  \u001b[39m=\u001b[39mdice_loss\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Experiments/SEGFormer/segformer_training.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdamW(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlr)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Experiments/SEGFormer/segformer_training.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m history \u001b[39m=\u001b[39m fit(epoch, model, train_batch,test_batch, criterion, optimizer)\n",
      "\u001b[1;32m/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Experiments/SEGFormer/segformer_training.ipynb Cell 18\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Experiments/SEGFormer/segformer_training.ipynb#X30sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m mask \u001b[39m=\u001b[39m mask_tiles\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mLongTensor)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Experiments/SEGFormer/segformer_training.ipynb#X30sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m#forward\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Experiments/SEGFormer/segformer_training.ipynb#X30sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m output \u001b[39m=\u001b[39m model(image)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Experiments/SEGFormer/segformer_training.ipynb#X30sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m#loss = criterion(output, mask.type(torch.LongTensor).to(device)).requires_grad_()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Experiments/SEGFormer/segformer_training.ipynb#X30sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m loss \u001b[39m=\u001b[39mcriterion(output, mask)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Experiments/SEGFormer/segformer_training.ipynb Cell 18\u001b[0m in \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Experiments/SEGFormer/segformer_training.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, pixel_values):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Experiments/SEGFormer/segformer_training.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# forward to get raw logits\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Experiments/SEGFormer/segformer_training.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     pixel_values\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39msqueeze(pixel_values, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Experiments/SEGFormer/segformer_training.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msegformer(pixel_values) \u001b[39m# what's the number of output channels?\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Experiments/SEGFormer/segformer_training.ipynb#X30sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     logits_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogits_outputs(outputs[\u001b[39m0\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Experiments/SEGFormer/segformer_training.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# next, do whatever you want with the logits to compute loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Experiments/SEGFormer/segformer_training.ipynb#X30sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# upsample logits here; the predicted distance map is also needed to unsampled, right?\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/LuminEye/lib/python3.10/site-packages/transformers/models/segformer/modeling_segformer.py:793\u001b[0m, in \u001b[0;36mSegformerForSemanticSegmentation.forward\u001b[0;34m(self, pixel_values, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    788\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    789\u001b[0m output_hidden_states \u001b[39m=\u001b[39m (\n\u001b[1;32m    790\u001b[0m     output_hidden_states \u001b[39mif\u001b[39;00m output_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39moutput_hidden_states\n\u001b[1;32m    791\u001b[0m )\n\u001b[0;32m--> 793\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msegformer(\n\u001b[1;32m    794\u001b[0m     pixel_values,\n\u001b[1;32m    795\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    796\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,  \u001b[39m# we need the intermediate hidden states\u001b[39;49;00m\n\u001b[1;32m    797\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    798\u001b[0m )\n\u001b[1;32m    800\u001b[0m encoder_hidden_states \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mhidden_states \u001b[39mif\u001b[39;00m return_dict \u001b[39melse\u001b[39;00m outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m    802\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode_head(encoder_hidden_states)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/LuminEye/lib/python3.10/site-packages/transformers/models/segformer/modeling_segformer.py:550\u001b[0m, in \u001b[0;36mSegformerModel.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    545\u001b[0m output_hidden_states \u001b[39m=\u001b[39m (\n\u001b[1;32m    546\u001b[0m     output_hidden_states \u001b[39mif\u001b[39;00m output_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39moutput_hidden_states\n\u001b[1;32m    547\u001b[0m )\n\u001b[1;32m    548\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 550\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    551\u001b[0m     pixel_values,\n\u001b[1;32m    552\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    553\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    554\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    555\u001b[0m )\n\u001b[1;32m    556\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    558\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/LuminEye/lib/python3.10/site-packages/transformers/models/segformer/modeling_segformer.py:427\u001b[0m, in \u001b[0;36mSegformerEncoder.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[39m# second, send embeddings through blocks\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[39mfor\u001b[39;00m i, blk \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(block_layer):\n\u001b[0;32m--> 427\u001b[0m     layer_outputs \u001b[39m=\u001b[39m blk(hidden_states, height, width, output_attentions)\n\u001b[1;32m    428\u001b[0m     hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    429\u001b[0m     \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/LuminEye/lib/python3.10/site-packages/transformers/models/segformer/modeling_segformer.py:334\u001b[0m, in \u001b[0;36mSegformerLayer.forward\u001b[0;34m(self, hidden_states, height, width, output_attentions)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states, height, width, output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 334\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    335\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer_norm_1(hidden_states),  \u001b[39m# in Segformer, layernorm is applied before self-attention\u001b[39;49;00m\n\u001b[1;32m    336\u001b[0m         height,\n\u001b[1;32m    337\u001b[0m         width,\n\u001b[1;32m    338\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    339\u001b[0m     )\n\u001b[1;32m    341\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    342\u001b[0m     outputs \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add self attentions if we output attention weights\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/LuminEye/lib/python3.10/site-packages/transformers/models/segformer/modeling_segformer.py:272\u001b[0m, in \u001b[0;36mSegformerAttention.forward\u001b[0;34m(self, hidden_states, height, width, output_attentions)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states, height, width, output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 272\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(hidden_states, height, width, output_attentions)\n\u001b[1;32m    274\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    275\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/LuminEye/lib/python3.10/site-packages/transformers/models/segformer/modeling_segformer.py:209\u001b[0m, in \u001b[0;36mSegformerEfficientSelfAttention.forward\u001b[0;34m(self, hidden_states, height, width, output_attentions)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m attention_scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(query_layer, key_layer\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m))\n\u001b[0;32m--> 209\u001b[0m attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m/\u001b[39;49m math\u001b[39m.\u001b[39;49msqrt(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention_head_size)\n\u001b[1;32m    211\u001b[0m \u001b[39m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[1;32m    212\u001b[0m attention_probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(attention_scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 5.81 GiB total capacity; 677.46 MiB already allocated; 10.00 MiB free; 698.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "epoch = 5\n",
    "lr = 0.00006\n",
    "criterion  =dice_loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "history = fit(epoch, model, train_batch,test_batch, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LuminEye",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
