{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dataclass Preparation\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import albumentations as A\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import time \n",
    "import mediapipe\n",
    "from torchvision import transforms\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import albumentations as A\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision import models\n",
    "import wandb\n",
    "import sys\n",
    "sys.path.append(\"../BaseModels\")\n",
    "from  unet_model import UNET\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = \"/home/nipun/Documents/Uni_Malta/Datasets/CenterRegression/MixDataset/images\"\n",
    "trn_df = pd.read_csv(\"/home/nipun/Documents/Uni_Malta/Datasets/CenterRegression/MixDataset/trainAll.csv\")\n",
    "val_df = pd.read_csv(\"/home/nipun/Documents/Uni_Malta/Datasets/CenterRegression/MixDataset/valAll.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv(input_channels,output_channels):\n",
    "    return nn.Sequential(nn.Conv2d(input_channels,output_channels,kernel_size=3,stride=1,padding='same'),\n",
    "                         nn.BatchNorm2d(output_channels),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Conv2d(output_channels,output_channels,kernel_size=3,stride=1,padding='same'),\n",
    "                         nn.BatchNorm2d(output_channels),\n",
    "                         nn.ReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.conv_1 = double_conv(3,64)\n",
    "        self.mx_1 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        \n",
    "        \n",
    "        self.conv_2 = double_conv(64,128)\n",
    "        self.mx_2 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        \n",
    "        \n",
    "        self.conv_3 = double_conv(128,256)\n",
    "        self.mx_3 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv_4 = double_conv(256,512)\n",
    "        self.mx_4 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv_5 = double_conv(512,1024)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.up_1 = nn.ConvTranspose2d(1024,512,kernel_size=2,stride=2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv6 = double_conv(1024,512)\n",
    "       \n",
    "       \n",
    "        self.up_2 = nn.ConvTranspose2d(512,256,kernel_size=2,stride=2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv7 = double_conv(512,256)\n",
    "        \n",
    "        \n",
    "        self.up_3 = nn.ConvTranspose2d(256,128,kernel_size=2,stride=2)\n",
    "        \n",
    "        \n",
    "        self.conv8 = double_conv(256,128)\n",
    "        \n",
    "\n",
    "        self.up_4 = nn.ConvTranspose2d(128,64,kernel_size=2,stride=2)\n",
    "        \n",
    "        self.conv9 = double_conv(128,64)\n",
    "        \n",
    "        self.out_conv = nn.Conv2d(64,self.num_classes,kernel_size=1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "\n",
    "        \n",
    "        x1 = self.conv_1(x)  # 1, 64, 64,64\n",
    "        \n",
    "        x1_max = self.mx_1(x1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x2 = self.conv_2(x1_max)  # 1, 128, 32, 32\n",
    "        x2_max = self.mx_2(x2)\n",
    "        \n",
    "        x3 = self.conv_3(x2_max)  # 1, 256, 16,16\n",
    "        x3_max = self.mx_3(x3)\n",
    "        \n",
    "        \n",
    "        x4 = self.conv_4(x3_max) # 1, 512, 8, 8\n",
    "        x4_max = self.mx_4(x4)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Bridge\n",
    "        \n",
    "        x5 = self.conv_5(x4_max)  # 1, 1024, 4, 4\n",
    "       \n",
    "        \n",
    "        \n",
    "        x6 = self.up_1(x5) # [1, 512, 8, 8]\n",
    "      \n",
    "        \n",
    "        \n",
    "        decoder_1 = torch.concat([x6,x4],dim=1) # 1, 1024, 8, 8]\n",
    "        \n",
    "        \n",
    "        decoder_1 = self.conv6(decoder_1)  # [1, 512, 8, 8]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        d_2 = self.up_2(decoder_1)\n",
    "        \n",
    "        decoder_2 = torch.concat([d_2,x3],dim=1) # [1, 512, 16, 16]\n",
    "        \n",
    "        decoder_2 = self.conv7(decoder_2) # [1, 256, 16, 16]\n",
    "        \n",
    "       \n",
    "        d_3 = self.up_3(decoder_2)\n",
    "        \n",
    "        decoder_3 = torch.concat([d_3,x2],dim=1)\n",
    "        \n",
    "        decoder_3 = self.conv8(decoder_3) # [1, 128, 32, 32]\n",
    "        \n",
    "        \n",
    "        d_4 = self.up_4(decoder_3)\n",
    "        decoder_4 = torch.concat([d_4,x1],dim=1)\n",
    "        \n",
    "        decoder_4 = self.conv9(decoder_4) # 1, 64, 64, 64\n",
    "        \n",
    "        output = self.out_conv(decoder_4)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true,y_pred):\n",
    "    \"\"\" Return MSE for the Batch\"\"\"\n",
    "    return torch.sum(torch.square(y_pred-y_true),axis=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CenterDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,df,image_dir=IMAGE_DIR,RESIZE_AMT=64):\n",
    "        \n",
    "        self.RESIZE_AMT = RESIZE_AMT\n",
    "        self.image_dir = image_dir\n",
    "        self.df = df\n",
    "        self.image_ids = df.Image_Name.unique()\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    # apply gaussian kernel to image\n",
    "    def _gaussian(self, xL, yL, sigma, H, W):\n",
    "\n",
    "        channel = [math.exp(-((c - xL) ** 2 + (r - yL) ** 2) / (2 * sigma ** 2)) for r in range(H) for c in range(W)]\n",
    "        channel = np.array(channel, dtype=np.float32)\n",
    "        channel = np.reshape(channel, newshape=(H, W))\n",
    "\n",
    "        return channel\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # convert original image to heatmap\n",
    "    def _convertToHM(self, img, keypoints, sigma=2):\n",
    "\n",
    "        H = img.shape[0] \n",
    "        W =  img.shape[1]\n",
    "        nKeypoints = len(keypoints)\n",
    "\n",
    "        img_hm = np.zeros(shape=(H, W, nKeypoints // 2), dtype=np.float32)\n",
    "\n",
    "        for i in range(0, nKeypoints // 2):\n",
    "            x = keypoints[i * 2]\n",
    "            y = keypoints[1 + 2 * i]\n",
    "\n",
    "            channel_hm = self._gaussian(x, y, sigma, H, W)\n",
    "\n",
    "            img_hm[:, :, i] = channel_hm\n",
    "        \n",
    "\n",
    "        return img_hm\n",
    "    def __getitem__(self,ix):\n",
    "        \n",
    "        img_id = self.image_ids[ix]\n",
    "        img_path = os.path.join(self.image_dir,img_id)\n",
    "        \n",
    "        img = cv2.imread(img_path)[:,:,::-1]\n",
    "        \n",
    "        \n",
    "        img = cv2.resize(img,(self.RESIZE_AMT,self.RESIZE_AMT))\n",
    "        \n",
    "        img = img/255.0\n",
    "        \n",
    "        \n",
    "        \n",
    "        data = self.df[self.df[\"Image_Name\"]==img_id]\n",
    "        \n",
    "        \n",
    "        x1 = data[\"X1\"].values[0] * self.RESIZE_AMT\n",
    "        y1 = data[\"Y1\"].values[0] * self.RESIZE_AMT\n",
    "        \n",
    "        \n",
    "        # heatmap = torch.tensor(self._convertToHM(img,[x1,y1]),dtype=torch.float32).permute(2,0,1).view(1*self.RESIZE_AMT*self.RESIZE_AMT)\n",
    "        \n",
    "        \n",
    "        heatmap = torch.tensor(self._convertToHM(img,[x1,y1]),dtype=torch.float32).permute(2,0,1)\n",
    "        \n",
    "        image = torch.tensor(img,dtype=torch.float32).permute(2,0,1)\n",
    "        \n",
    "        \n",
    "        return image,heatmap\n",
    "        \n",
    "    def collate_fn(self,batch):\n",
    "        return tuple(zip(*batch))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = CenterDataset(trn_df)\n",
    "test_ds = CenterDataset(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img,hm = train_ds[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(torch.unique(hm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACTH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader = DataLoader(train_ds, batch_size=BACTH_SIZE,\n",
    "\tshuffle=True, num_workers=os.cpu_count(), pin_memory=True,drop_last=True)\n",
    "testLoader = DataLoader(test_ds, batch_size=BACTH_SIZE,\n",
    "\tnum_workers=os.cpu_count(), pin_memory=True,drop_last=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(trainLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [[2,2],\n",
    "          [3,3]]\n",
    "\n",
    "y_pred = [[4,4],\n",
    "          [6,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.tensor(y_true)\n",
    "\n",
    "x2 = torch.tensor(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.sum(torch.square(y_pred-y_true),axis=-1).mean()\n",
    "\n",
    "torch.square(x2-x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.sum(torch.square(x2-x1),axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# torch.log  and math.log is e based\n",
    "class AdaptiveWingLoss(nn.Module):\n",
    "    def __init__(self, omega=14, theta=0.5, epsilon=1, alpha=2.1):\n",
    "        super(AdaptiveWingLoss, self).__init__()\n",
    "        self.omega = omega\n",
    "        self.theta = theta\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        '''\n",
    "        :param pred: BxNxHxH\n",
    "        :param target: BxNxHxH\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        y = target\n",
    "        y_hat = pred\n",
    "        delta_y = (y - y_hat).abs()\n",
    "        delta_y1 = delta_y[delta_y < self.theta]\n",
    "        delta_y2 = delta_y[delta_y >= self.theta]\n",
    "        y1 = y[delta_y < self.theta]\n",
    "        y2 = y[delta_y >= self.theta]\n",
    "        loss1 = self.omega * torch.log(1 + torch.pow(delta_y1 / self.omega, self.alpha - y1))\n",
    "        A = self.omega * (1 / (1 + torch.pow(self.theta / self.epsilon, self.alpha - y2))) * (self.alpha - y2) * (\n",
    "            torch.pow(self.theta / self.epsilon, self.alpha - y2 - 1)) * (1 / self.epsilon)\n",
    "        C = self.theta * A - self.omega * torch.log(1 + torch.pow(self.theta / self.epsilon, self.alpha - y2))\n",
    "        loss2 = A * delta_y2 - C\n",
    "        return (loss1.sum() + loss2.sum()) / (len(loss1) + len(loss2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = AdaptiveWingLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = UNET(1).cuda()\n",
    "pred = model(x.cuda())\n",
    "\n",
    "\n",
    "# pred_sig = sigmoid(pred)\n",
    "\n",
    "# pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1 = pred_sig[2].squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(pred_1.shape)\n",
    "\n",
    "plt.imshow(pred_1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(pred[0],y[0].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(pred[1],y[1].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(pred[2],y[2].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(pred[3],y[3].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(pred,y.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model,trainLoader,testLoader,optimizer,loss_fn,scheduler,epochs=100,):\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            train_epoch_loss = trainStep(model,trainLoader,optimizer,loss_fn)\n",
    "            val_epoch_loss=valStep(model,testLoader,loss_fn)\n",
    "            \n",
    "            scheduler.step(val_epoch_loss)\n",
    "            \n",
    "            \n",
    "            print(f\"Epoch {epoch+1}| Train AdaptiveWing Loss--> {train_epoch_loss}\")\n",
    "            print(f\"Epoch {epoch+1}| VAL Adaptive Loss--> {val_epoch_loss}\")\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainStep(model,trainLoader,optimizer,loss_fn):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    total_step = 0\n",
    "    \n",
    "    for _,(x,y) in enumerate(trainLoader):\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        \n",
    "        \n",
    "        loss = loss_fn(y_pred,y)\n",
    "        # loss = mean_squared_error(y,y_pred)\n",
    "        \n",
    "        # print(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # total_step += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        total_step += x[0]\n",
    "        \n",
    "    return epoch_loss\n",
    "    # print(epoch_loss/total_step)\n",
    "    # return epoch_loss/total_step\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainStep(model,testLoader,optimizer,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valStep(model,testLoader,loss_fn,visualize=True):\n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "    total_val_loss = 0\n",
    "   \n",
    "    \n",
    "    total_step = 0\n",
    "    \n",
    "    for (x,y) in testLoader:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            y_pred = model(x)\n",
    "        \n",
    "        #MSE\n",
    "        # loss = mean_squared_error(y,y_pred).item()\n",
    "        \n",
    "        loss = loss_fn(y_pred,y)\n",
    "        \n",
    "        total_val_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        total_step += x.shape[0]\n",
    "  \n",
    "    fig,axes = plt.subplots(x.shape[0],3)\n",
    "    axes[0][0].set_title(\"Image\")  \n",
    "    axes[0][1].set_title(\"GT HeatMap\")\n",
    "    axes[0][2].set_title(\"Pred HeatMap\")\n",
    "    for i in range(x.shape[0]):\n",
    "        \n",
    "        img = x[i].squeeze(0).permute(1,2,0).detach().cpu().numpy()\n",
    "        gt = y[i].squeeze(0).detach().cpu().numpy()\n",
    "        heatMap = y_pred[i].squeeze(0).detach().cpu().numpy()\n",
    "        \n",
    "        \n",
    "        axes[i][0].imshow(img)\n",
    "        axes[i][1].imshow(gt)\n",
    "        axes[i][2].imshow(heatMap)\n",
    "        \n",
    "        \n",
    "        axes[i][0].axis('off')\n",
    "        axes[i][1].axis('off')\n",
    "        axes[i][2].axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(\"all\")\n",
    "    \n",
    "    return total_val_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valStep(model,testLoader,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskToKeypoints(mask):\n",
    "    \n",
    "    \n",
    "    \n",
    "    kp = np.unravel_index(np.argmax(mask, axis=None), dims=(64,64))\n",
    "    return kp[1], kp[0]\n",
    "\n",
    "def findCoordinates(mask):\n",
    "\n",
    "    hm_sum = np.sum(mask)\n",
    "\n",
    "    index_map = [j for i in range(64) for j in range(64)]\n",
    "    index_map = np.reshape(index_map, newshape=(64,64))\n",
    "\n",
    "    x_score_map = mask * index_map / hm_sum\n",
    "    y_score_map = mask * np.transpose(index_map) / hm_sum\n",
    "\n",
    "    px = np.sum(np.sum(x_score_map, axis=None))\n",
    "    py = np.sum(np.sum(y_score_map, axis=None))\n",
    "\n",
    "    return px, py\n",
    "\n",
    "def calcRMSError(kps_gt, kps_preds):\n",
    "\n",
    "    N = kps_gt.shape[0] * (kps_gt.shape[-1] // 2)\n",
    "    error = np.sqrt(np.sum((kps_gt-kps_preds)**2)/N)\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNET(1).to(device)\n",
    "\n",
    "lr = 0.001\n",
    "criterion = AdaptiveWingLoss()\n",
    "\n",
    "n_epoch = 100\n",
    "\n",
    "parameters = filter(lambda p: p.requires_grad,model.parameters())\n",
    "\n",
    "optimizer = torch.optim.Adam(parameters,lr=lr)\n",
    "\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        lr,\n",
    "        n_epoch,\n",
    "        steps_per_epoch=len(train_ds)//BACTH_SIZE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(model,trainLoader,testLoader,optimizer,loss_fn=criterion,scheduler=scheduler,epochs=n_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(testLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_output = model(x.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unflatten = nn.Unflatten(-1,(1,64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_output = unflatten(pred_output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pred_output[idx].permute(1,2,0).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = unflatten(y)\n",
    "\n",
    "plt.imshow(y_true[idx].permute(1,2,0).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1.,2.],\n",
    "                  [3.,4.]])\n",
    "\n",
    "\n",
    "y =  torch.tensor([[4.,5.],\n",
    "                  [1.,2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcKeypoints(model, gen):\n",
    "    kps_gt = []\n",
    "    kps_preds = []\n",
    "    \n",
    "    image_array = []\n",
    "    gt_mask_array =[]\n",
    "    pred_mask_array =[]\n",
    "    nbatches = len(gen)\n",
    "    \n",
    "    unflatter = nn.Unflatten(1,(64,64))\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for (x,y) in gen:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y= y.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x)\n",
    "            \n",
    "        for i in range(x.shape[0]):\n",
    "            \n",
    "            \n",
    "            imgs = x[i].detach().cpu().permute(1,2,0).numpy()\n",
    "            \n",
    "            mask_gt = y[i]\n",
    "            \n",
    "            mask_gt = unflatten(mask_gt).detach().cpu().permute(1,2,0).numpy()\n",
    "            \n",
    "           \n",
    "            \n",
    "            mask_pred = y_pred[i]\n",
    "            \n",
    "            mask_pred = unflatten(mask_pred).detach().cpu().permute(1,2,0).numpy()\n",
    "\n",
    "\n",
    "            \n",
    "            xgt, ygt = findCoordinates(mask_gt[:, :,0])\n",
    "            \n",
    "            xpred, ypred = findCoordinates(mask_pred[:, :,0])\n",
    "            \n",
    "\n",
    "            image_array.append(imgs)\n",
    "            gt_mask_array.append(mask_gt)\n",
    "            pred_mask_array.append(mask_pred)\n",
    "                        \n",
    "            kps_gt.append([xgt,ygt])\n",
    "            kps_preds.append([xpred,ypred])\n",
    "    \n",
    "    \n",
    "    return image_array, gt_mask_array, pred_mask_array,kps_gt,kps_preds\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# len(testLoader)\n",
    "\n",
    "image_array, gt_mask_array, pred_mask_array,kps_gt,kps_preds = calcKeypoints(model, testLoader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(kps_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showMasks(imgs, gt_masks, pred_masks, gt_coord, pred_coord, nrows=8, ncols=4):\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(25, 25))\n",
    "    plt.figure(figsize=(8, 8))\n",
    "\n",
    "    axs[0, 0].set_title(\"Original Image\")\n",
    "    axs[0, 1].set_title(\"GT Masks\")\n",
    "    axs[0, 2].set_title(\"Pred Masks\")\n",
    "    axs[0, 3].set_title(\"Pred & GT Coord\")\n",
    "\n",
    "\n",
    "    for i in range(len(imgs)):\n",
    "\n",
    "        axs[i, 0].imshow(imgs[i])\n",
    "        axs[i, 0].axis(\"off\")\n",
    "\n",
    "        axs[i, 1].imshow(gt_masks[i])\n",
    "        axs[i, 1].axis(\"off\")\n",
    "\n",
    "        axs[i, 2].imshow(pred_masks[i])\n",
    "        axs[i, 2].axis(\"off\")\n",
    "\n",
    "        axs[i, 3].imshow(imgs[i])\n",
    "        axs[i, 3].scatter(x=int(pred_coord[i][0]), y=int(\n",
    "            pred_coord[i][1]), color='blue')  # Prediction Blue\n",
    "        axs[i, 3].scatter(x=int(gt_coord[i][0]), y=int(\n",
    "            gt_coord[i][1]), color=\"red\")  # Ground Truth Red\n",
    "        axs[i, 3].axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_limit = 50\n",
    "upper_limit = 58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showMasks(image_array[low_limit:upper_limit], gt_mask_array[low_limit:upper_limit], pred_mask_array[low_limit:upper_limit],kps_gt[low_limit:upper_limit],kps_preds[low_limit:upper_limit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WingLoss(nn.Module):\n",
    "    def __init__(self, width=5, curvature=0.5):\n",
    "        super(WingLoss, self).__init__()\n",
    "        self.width = width\n",
    "        self.curvature = curvature\n",
    "        self.C = self.width - self.width * np.log(1 + self.width / self.curvature)\n",
    "\n",
    "    def forward(self, prediction, target):\n",
    "        diff = target - prediction\n",
    "        \n",
    "        diff_abs = diff.abs()\n",
    "        \n",
    "        loss = diff_abs.clone()\n",
    "        \n",
    "\n",
    "        idx_smaller = diff_abs < self.width\n",
    "        idx_bigger = diff_abs >= self.width\n",
    "        \n",
    "        \n",
    "        loss[idx_smaller] = self.width * torch.log(1 + diff_abs[idx_smaller] / self.curvature)\n",
    "        loss[idx_bigger]  = loss[idx_bigger] - self.C\n",
    "        \n",
    "        \n",
    "    \n",
    "        return  loss\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1.,2.],\n",
    "                  [3.,4.]])\n",
    "\n",
    "\n",
    "y = torch.tensor([[4.,5.],\n",
    "                  [6.,8.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wingLoss = WingLoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = wingLoss(y,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss.sum(1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z<5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z>=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Adaptive Wing Loss analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import  Dataset,DataLoader\n",
    "from torchvision import transforms \n",
    "from scipy import interpolate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from scipy import  ndimage\n",
    "from scipy.ndimage.morphology import  grey_dilation\n",
    "import math\n",
    "from PIL import  Image\n",
    "import  warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class face300w(Dataset):\n",
    "    \n",
    "    def __init__(self,root):\n",
    "        self.root = root\n",
    "        entry_point = os.listdir(root)\n",
    "        print(entry_point)\n",
    "        self.ptsPath = list()\n",
    "        self.imgPath = list()\n",
    "        \n",
    "        self.crop_pad = 30\n",
    "        \n",
    "        \n",
    "        for i in entry_point:\n",
    "            files = os.listdir(root+\"/\"+i)\n",
    "            \n",
    "            for f in files:\n",
    "                \n",
    "                if (f[-3:] == \"png\"):\n",
    "                    self.imgPath.append(i+\"/\"+f)\n",
    "                    \n",
    "                elif (f[-3:] == \"pts\"):\n",
    "                    self.ptsPath.append(i+\"/\"+f)\n",
    "                else:\n",
    "                    print(\"WrongFormat\"\n",
    "                          )\n",
    "                    \n",
    "        self.ptsPath.sort()\n",
    "        self.imgPath.sort()\n",
    "        self.frame = True\n",
    "        self.resize = 256\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        imgPath = self.root + \"/\" + self.imgPath[idx]\n",
    "        ptsPath = self.root + \"/\" + self.ptsPath[idx]\n",
    "        \n",
    "        img = plt.imread(imgPath) #RGT\n",
    "        # print(img.shape)\n",
    "        # plt.imshow(img)\n",
    "        # plt.show()\n",
    "        \n",
    "        \n",
    "        if (len(img.shape)==2):\n",
    "            \n",
    "            # gray to RGB\n",
    "            img = img.reshape(img.shape[0],img.shape[1],1)\n",
    "            img = np.repeat(img,3,axis=2)\n",
    "            \n",
    "        w,h,c = img.shape\n",
    "        \n",
    "        with open(ptsPath) as ptsf:\n",
    "            rows = [rows.strip() for rows in ptsf][3:-1]\n",
    "            \n",
    "            if len(rows)!= 68:\n",
    "                print(\"points are not 68\")\n",
    "                return None\n",
    "            \n",
    "            toFloat = lambda lst: [float(i) for i in lst] \n",
    "            \n",
    "            rows =[toFloat(pair.split(\" \")) for pair in rows]\n",
    "            \n",
    "            rows = np.array(rows)\n",
    "            \n",
    "            \n",
    "            minx,maxx = rows[:,0].min(),rows[:,0].max()\n",
    "            miny,maxy = rows[:,1].min(),rows[:,1].max()\n",
    "            \n",
    "            face_h = maxx-minx\n",
    "            \n",
    "            # plt.imsho\n",
    "            \n",
    "            if(self.frame):\n",
    "                csh = img.shape #534 950\n",
    "                print(csh)\n",
    "                frame = np.zeros((max(csh[0],csh[1]),max(csh[0],csh[1]),3)) # 950 950\n",
    "                \n",
    "                \n",
    "                frame_ctr = np.array([max(csh[0],csh[1])//2,max(csh[0],csh[1])//2]) # [475,475]\n",
    "                \n",
    "                \n",
    "                frame[math.ceil(frame_ctr[0]-csh[0]/2.):math.ceil(frame_ctr[0]+csh[0]/2.),\n",
    "                  math.ceil(frame_ctr[1]-csh[1]/2.):math.ceil(frame_ctr[1]+csh[1]/2.),:] = img\n",
    "                \n",
    "                if(csh[1] != frame.shape[1]):\n",
    "              \n",
    "                    rows[:,0] += (frame.shape[0]-csh[1])/2.\n",
    "                else:\n",
    "                    #\n",
    "                    rows[:,1] += (frame.shape[0]-csh[0])/2.\n",
    "\n",
    "\n",
    "                if(self.resize != None):\n",
    "                    rows /= frame.shape[0]\n",
    "                    frame = cv2.resize(frame, dsize=(self.resize, self.resize), interpolation=cv2.INTER_LINEAR)\n",
    "                    rows *= float(self.resize)\n",
    "                    \n",
    "                    \n",
    "                hmap = np.zeros((68+1,64,64),dtype=np.float32)\n",
    "                \n",
    "                M = np.zeros((68+1,64,64),dtype=np.float32)\n",
    "                \n",
    "                \n",
    "                for ind,xy in enumerate(rows):\n",
    "                    hmap[ind] = draw_umich_gaussian(hmap[ind], xy/256.*64, 7)\n",
    "                    \n",
    "                hmap[-1] = draw_boundary(hmap[-1],np.clip((rows/256.*64).astype(np.int),0,63))\n",
    "\n",
    "                for i in range(len(M)):\n",
    "                    \n",
    "                    M[i] = grey_dilation(hmap[i], size=(3,3))\n",
    "                M = np.where(M>=0.5, 1, 0)\n",
    "\n",
    "                return frame, hmap , M, rows\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.ceil(475 +950/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/home/nipun/Downloads/300w/\"\n",
    "\n",
    "\n",
    "dataset = face300w(dataset_dir)\n",
    "\n",
    "img,hmap,M,pts = dataset[45]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img.shape # (256, 256, 3)\n",
    "plt.imshow(hmap[68]) # (69,69)\n",
    "# hmap[9].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian2D(shape, sigma=1):\n",
    "    \n",
    "    m, n = [(ss - 1.) / 2. for ss in shape]\n",
    "    y, x = np.ogrid[-m:m + 1, -n:n + 1]\n",
    "\n",
    "    h = np.exp(-(x * x + y * y) / (2 * sigma * sigma))\n",
    "    h[h < np.finfo(h.dtype).eps * h.max()] = 0\n",
    "    return h\n",
    "\n",
    "def draw_umich_gaussian(heatmap, center, radius, k=1):\n",
    "    \n",
    "    diameter = 2 * radius + 1\n",
    "    gaussian = gaussian2D((diameter, diameter), sigma=diameter / 6)\n",
    "\n",
    "    x, y = int(center[0]), int(center[1])\n",
    "\n",
    "    height, width = heatmap.shape[0:2]\n",
    "\n",
    "    left, right = min(x, radius), min(width - x, radius + 1)\n",
    "    top, bottom = min(y, radius), min(height - y, radius + 1)\n",
    "\n",
    "    masked_heatmap = heatmap[y - top:y + bottom, x - left:x + right]\n",
    "    masked_gaussian = gaussian[radius - top:radius + bottom, radius - left:radius + right]\n",
    "    if min(masked_gaussian.shape) > 0 and min(masked_heatmap.shape) > 0:  # TODO debug\n",
    "        np.maximum(masked_heatmap, masked_gaussian * k, out=masked_heatmap)\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(3,1,1)\n",
    "plt.imshow(img)\n",
    "plt.imshow(cv2.resize(hmap[-1], dsize=(256, 256), interpolation=cv2.INTER_AREA),alpha=0.3)\n",
    "plt.subplot(3,1,2)\n",
    "plt.imshow(np.max(hmap[:68], axis=0))\n",
    "plt.subplot(3,1,3)\n",
    "plt.imshow(np.max(M[:68], axis=0))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawline(img,annots):\n",
    "    shape = img.shape\n",
    "    points_cnt = annots.shape[0]\n",
    "    heatmap = np.zeros((shape[0],shape[1]))\n",
    "    \n",
    "    heatmap[[list(annots[:,1]),list(annots[:,0])]] = 1\n",
    "    \"\"\"\n",
    "    for p in range(points_cnt-1):\n",
    "        heatmap = cv2.line(heatmap,tuple(annots[p,:]),tuple(annots[p+1,:]),(255,255,255),thickness=5)\n",
    "\n",
    "    heatmap = heatmap.astype(np.float32)\n",
    "    annots = annots.astype(np.int)\n",
    "    heatmap = cv2.polylines(heatmap, [annots], False, (255,255,255), thickness=3, lineType=cv2.LINE_AA)\n",
    "    \"\"\"\n",
    "\n",
    "    annots = annots.astype(np.float64)\n",
    "    x,y = annots[:,0],annots[:,1]\n",
    "    l=len(x)\n",
    "    t=np.linspace(0,1,l-2,endpoint=True)\n",
    "    t=np.append([0,0,0],t)\n",
    "    t=np.append(t,[1,1,1])\n",
    "    tck=[t,[x,y],3]\n",
    "    u3=np.linspace(0,1,(max(l*2,500)),endpoint=True)\n",
    "    out = interpolate.splev(u3,tck)\n",
    "    heatmap[[list(out[1].astype(np.int)),list(out[0].astype(np.int))]] = 1\n",
    "\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boundary(img,annot):\n",
    "    heatmap = np.zeros((64,64))\n",
    "\n",
    "    heatmap += drawline(img,annot[0:17,:])\n",
    "    heatmap += drawline(img,annot[17:22,:])\n",
    "    heatmap += drawline(img,annot[22:27,:])\n",
    "    heatmap += drawline(img,annot[36:40,:])\n",
    "    heatmap += drawline(img, np.array([annot[i] for i in [36, 41, 40, 39]]))\n",
    "    heatmap += drawline(img,annot[42:46,:])\n",
    "    heatmap += drawline(img, np.array([annot[i] for i in [42, 47, 46, 45]]))\n",
    "    heatmap += drawline(img,annot[27:31,:])\n",
    "    heatmap += drawline(img,annot[31:36,:])\n",
    "    heatmap += drawline(img,annot[48:55,:])\n",
    "    heatmap += drawline(img, np.array([annot[i] for i in [60, 61, 62, 63, 64]]))\n",
    "    heatmap += drawline(img, np.array([annot[i] for i in [48, 59, 58, 57, 56, 55, 54]]))\n",
    "    heatmap += drawline(img, np.array([annot[i] for i in [60, 67, 66, 65, 64]]))\n",
    "    heatmap = np.clip(heatmap,0,1).astype(np.uint8)*255\n",
    "\n",
    "    heatmap = 255-heatmap\n",
    "    dist_transform = cv2.distanceTransform(heatmap, cv2.DIST_L2, cv2.DIST_MASK_PRECISE)\n",
    "    dist_transform = dist_transform.astype(np.float64)\n",
    "    sigma = 1\n",
    "    gt = np.where(dist_transform < 3*sigma, np.exp(-(dist_transform*dist_transform)/(2*sigma*sigma)), 0 )\n",
    "\n",
    "    return gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[1,2,3],\n",
    "     [4,5,6]]\n",
    "\n",
    "\n",
    "\n",
    "row = len(x) 2 3 --> 3 2\n",
    "column = len(x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_array = [ [0] * row for _ in range(column)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "column = 0\n",
    "for row_idx,row in enumerate(x):\n",
    "    \n",
    "  \n",
    "\n",
    "    while column<=2:\n",
    "        print(column)\n",
    "        \n",
    "        zeros_array[column ][row_idx] = row[column]\n",
    "        column +=1\n",
    "    column = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = nn.Conv2d(3,6,kernel_size=3,padding=1\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = tf.expand_dims(tf.range([6]),0) # 1,6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_range = tf.tile(x_range,[2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= tf.random.uniform(shape=[2,28,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.random.uniform(shape=[2,1,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tf.matmul(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5!\n",
    "\n",
    "\n",
    "5--> 5*4*3*2*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorial(n):\n",
    "    \n",
    "    if n==1:\n",
    "        return 1\n",
    "    return n *factorial(n-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self,val):\n",
    "        self.val = val\n",
    "        self.next = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = LinkedList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1.insert(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1.insert(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1.insert(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1.addBegining(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1.addMiddle(1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import albumentations as A\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import time \n",
    "import mediapipe\n",
    "from torchvision import transforms\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import albumentations as A\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision import models\n",
    "import wandb\n",
    "import sys\n",
    "sys.path.append(\"../BaseModels\")\n",
    "from  unet_model import UNET\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = \"cpu\"\n",
    "\n",
    "RESIZE_AMT = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# '''\n",
    "#      Instantiate the object like:\n",
    "#           criterion = AdaptiveWingLoss(whetherWeighted=True)\n",
    "#      #param whetherWeighted: whether use weighted loss map\n",
    "#      #param dilaStru: size of dilation structure\n",
    "# '''\n",
    "class AdaptiveWingLoss(nn.Module):\n",
    "  def __init__(self, alpha=2.1, omega=14.0, theta=0.5, epsilon=1.0,\\\n",
    "               whetherWeighted=False, dilaStru=3, w=10, device=device):\n",
    "    super(AdaptiveWingLoss, self).__init__()\n",
    "    self.device = device\n",
    "    self.alpha = torch.Tensor([alpha]).to(device)\n",
    "    self.omega = torch.Tensor([omega]).to(device)\n",
    "    self.theta = torch.Tensor([theta]).to(device)\n",
    "    self.epsilon = torch.Tensor([epsilon]).to(device)\n",
    "    self.dilationStru = dilaStru\n",
    "    self.w = torch.Tensor([w]).to(device)\n",
    "    self.tmp = torch.Tensor([self.theta / self.epsilon]).to(device)\n",
    "    self.wetherWeighted = whetherWeighted\n",
    "\n",
    "# '''\n",
    "#    #param predictions: predicted heat map with dimension of batchSize * landmarkNum * heatMapSize * heatMapSize  \n",
    "#    #param targets: ground truth heat map with dimension of batchSize * landmarkNum * heatMapSize * heatMapSize  \n",
    "# '''\n",
    "  def forward(self, predictions, targets):\n",
    "    \n",
    "    \n",
    "    deltaY = predictions - targets\n",
    "    deltaY = torch.abs(deltaY)\n",
    "    alphaMinusY = self.alpha - targets\n",
    "    \n",
    "    \n",
    "    a = self.omega / self.epsilon * alphaMinusY / (1 + self.tmp.pow(alphaMinusY))\\\n",
    "        * self.tmp.pow(alphaMinusY - 1)\n",
    "    c = self.theta * a - self.omega * torch.log(1 + self.tmp.pow(alphaMinusY))\n",
    "\n",
    "    l = torch.where(deltaY < self.theta,\n",
    "                    self.omega * torch.log(1 + (deltaY / self.epsilon).pow(alphaMinusY)),\n",
    "                    a * deltaY - c)\n",
    "    if self.wetherWeighted:\n",
    "      weightMap = self.grayDilation(targets, self.dilationStru)\n",
    "      weightMap = torch.where(weightMap >= 0.2, torch.Tensor([1]).to(self.device),\\\n",
    "                              torch.Tensor([0]).to(self.device))\n",
    "      l = l * (self.w * weightMap + 1)\n",
    "\n",
    "    l = torch.mean(l)\n",
    "\n",
    "    return l\n",
    "    \n",
    "  def grayDilation(self, heatmapGt, structureSize):\n",
    "    batchSize, landmarkNum, heatmapSize, _ = heatmapGt.shape\n",
    "    weightMap = heatmapGt.clone()\n",
    "    step = structureSize // 2\n",
    "    for i in range(1, heatmapSize-1, 1):\n",
    "      for j in range(1, heatmapSize-1, 1):\n",
    "        weightMap[:, :, i, j] = torch.max(heatmapGt[:, :, i - step: i + step + 1,\\\n",
    "                                j - step: j + step + 1].contiguous().view(batchSize,\\\n",
    "                                landmarkNum, structureSize * structureSize), dim=2)[0]\n",
    "\n",
    "    return weightMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMAGE_DIR = \"/home/nipun/Documents/Uni_Malta/Datasets/CenterRegression/MixDataset/images\"\n",
    "trn_df = pd.read_csv(\"/home/nipun/Documents/Uni_Malta/Datasets/CenterRegression/MixDataset/trainAll.csv\")\n",
    "val_df = pd.read_csv(\"/home/nipun/Documents/Uni_Malta/Datasets/CenterRegression/MixDataset/valAll.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenterDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,df,image_dir=IMAGE_DIR,RESIZE_AMT=RESIZE_AMT):\n",
    "        \n",
    "        self.RESIZE_AMT = RESIZE_AMT\n",
    "        self.image_dir = image_dir\n",
    "        self.df = df\n",
    "        self.image_ids = df.Image_Name.unique()\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    # apply gaussian kernel to image\n",
    "    def _gaussian(self, xL, yL, sigma, H, W):\n",
    "\n",
    "        channel = [math.exp(-((c - xL) ** 2 + (r - yL) ** 2) / (2 * sigma ** 2)) for r in range(H) for c in range(W)]\n",
    "        channel = np.array(channel, dtype=np.float32)\n",
    "        channel = np.reshape(channel, newshape=(H, W))\n",
    "\n",
    "        return channel\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # convert original image to heatmap\n",
    "    def _convertToHM(self, img, keypoints, sigma=2):\n",
    "\n",
    "        H = img.shape[0] \n",
    "        W =  img.shape[1]\n",
    "        nKeypoints = len(keypoints)\n",
    "\n",
    "        img_hm = np.zeros(shape=(H, W, nKeypoints // 2), dtype=np.float32)\n",
    "\n",
    "        for i in range(0, nKeypoints // 2):\n",
    "            x = keypoints[i * 2]\n",
    "            y = keypoints[1 + 2 * i]\n",
    "\n",
    "            channel_hm = self._gaussian(x, y, sigma, H, W)\n",
    "\n",
    "            img_hm[:, :, i] = channel_hm\n",
    "            \n",
    "        return img_hm\n",
    "    def __getitem__(self,ix):\n",
    "        \n",
    "        img_id = self.image_ids[ix]\n",
    "        img_path = os.path.join(self.image_dir,img_id)\n",
    "        \n",
    "        img = cv2.imread(img_path)[:,:,::-1]\n",
    "        \n",
    "        \n",
    "        img = cv2.resize(img,(self.RESIZE_AMT,self.RESIZE_AMT))\n",
    "        \n",
    "        img = img/255.0\n",
    "        \n",
    "        \n",
    "        \n",
    "        data = self.df[self.df[\"Image_Name\"]==img_id]\n",
    "        \n",
    "        \n",
    "        x1 = data[\"X1\"].values[0] * self.RESIZE_AMT\n",
    "        y1 = data[\"Y1\"].values[0] * self.RESIZE_AMT\n",
    "        \n",
    "        \n",
    "        heatmap = torch.tensor(self._convertToHM(img,[x1,y1]),dtype=torch.float32).permute(2,0,1)#.view(1*self.RESIZE_AMT*self.RESIZE_AMT)\n",
    "        \n",
    "        image = torch.tensor(img,dtype=torch.float32).permute(2,0,1)\n",
    "        \n",
    "        \n",
    "        return image,heatmap\n",
    "        \n",
    "    def collate_fn(self,batch):\n",
    "        return tuple(zip(*batch))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACTH_SIZE = 512\n",
    "train_ds = CenterDataset(trn_df)\n",
    "test_ds = CenterDataset(val_df)\n",
    "\n",
    "trainLoader = DataLoader(train_ds, batch_size=BACTH_SIZE,\n",
    "\tshuffle=True, num_workers=os.cpu_count(), pin_memory=True,drop_last=True)\n",
    "testLoader = DataLoader(test_ds, batch_size=BACTH_SIZE,\n",
    "\tnum_workers=os.cpu_count(), pin_memory=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(trainLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap = y[0].permute(1,2,0).detach().cpu().numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize, landmarkNum, heatmapSize, _ = y.shape\n",
    "\n",
    "weightMap = y.clone()\n",
    "structureSize=3\n",
    "step = structureSize // 2\n",
    "for i in range(1, heatmapSize-1, 1):\n",
    "  for j in range(1, heatmapSize-1, 1):\n",
    "    weightMap[:, :, i, j] = torch.max(y[:, :, i - step: i + step + 1,\\\n",
    "                            j - step: j + step + 1].contiguous().view(batchSize,\\\n",
    "                            landmarkNum, structureSize * structureSize), dim=2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dilated_mask = weightMap[1].permute(1,2,0).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "plt.imshow(dilated_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def double_conv(input_channels,output_channels):\n",
    "    return nn.Sequential(nn.Conv2d(input_channels,output_channels,kernel_size=3,stride=1,padding='same'),\n",
    "                         nn.BatchNorm2d(output_channels),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Conv2d(output_channels,output_channels,kernel_size=3,stride=1,padding='same'),\n",
    "                         nn.BatchNorm2d(output_channels),\n",
    "                         nn.ReLU())\n",
    "    \n",
    "    \n",
    "class UNET(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.conv_1 = double_conv(3,64)\n",
    "        self.mx_1 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        \n",
    "        \n",
    "        self.conv_2 = double_conv(64,128)\n",
    "        self.mx_2 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        \n",
    "        \n",
    "        self.conv_3 = double_conv(128,256)\n",
    "        self.mx_3 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv_4 = double_conv(256,512)\n",
    "        self.mx_4 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv_5 = double_conv(512,1024)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.up_1 = nn.ConvTranspose2d(1024,512,kernel_size=2,stride=2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv6 = double_conv(1024,512)\n",
    "       \n",
    "       \n",
    "        self.up_2 = nn.ConvTranspose2d(512,256,kernel_size=2,stride=2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv7 = double_conv(512,256)\n",
    "        \n",
    "        \n",
    "        self.up_3 = nn.ConvTranspose2d(256,128,kernel_size=2,stride=2)\n",
    "        \n",
    "        \n",
    "        self.conv8 = double_conv(256,128)\n",
    "        \n",
    "\n",
    "        self.up_4 = nn.ConvTranspose2d(128,64,kernel_size=2,stride=2)\n",
    "        \n",
    "        self.conv9 = double_conv(128,64)\n",
    "        \n",
    "        self.out_conv = nn.Conv2d(64,self.num_classes,kernel_size=1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "\n",
    "        \n",
    "        x1 = self.conv_1(x)  # 1, 64, 64,64\n",
    "        \n",
    "        x1_max = self.mx_1(x1)\n",
    "        \n",
    "        #################\n",
    "        \n",
    "        x2 = self.conv_2(x1_max)  # 1, 128, 32, 32\n",
    "        x2_max = self.mx_2(x2)\n",
    "        #################\n",
    "        x3 = self.conv_3(x2_max)  # 1, 256, 16,16\n",
    "        x3_max = self.mx_3(x3)\n",
    "        ################\n",
    "        \n",
    "        x4 = self.conv_4(x3_max) # 1, 512, 8, 8\n",
    "        x4_max = self.mx_4(x4)\n",
    "        \n",
    "        #################\n",
    "        \n",
    "        # Bridge\n",
    "        \n",
    "        x5 = self.conv_5(x4_max)  # 1, 1024, 4, 4\n",
    "       \n",
    "        ####################\n",
    "        \n",
    "        x6 = self.up_1(x5) # [1, 512, 8, 8]\n",
    "      \n",
    "        \n",
    "        \n",
    "        decoder_1 = torch.concat([x6,x4],dim=1) # 1, 1024, 8, 8\n",
    "        \n",
    "        \n",
    "        decoder_1 = self.conv6(decoder_1)  # [1, 512, 8, 8]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        d_2 = self.up_2(decoder_1)\n",
    "        \n",
    "        decoder_2 = torch.concat([d_2,x3],dim=1) # [1, 512, 16, 16]\n",
    "        \n",
    "        decoder_2 = self.conv7(decoder_2) # [1, 256, 16, 16]\n",
    "        \n",
    "       \n",
    "        d_3 = self.up_3(decoder_2)\n",
    "        \n",
    "        decoder_3 = torch.concat([d_3,x2],dim=1)\n",
    "        \n",
    "        decoder_3 = self.conv8(decoder_3) # [1, 128, 32, 32]\n",
    "        \n",
    "        \n",
    "        d_4 = self.up_4(decoder_3)\n",
    "        decoder_4 = torch.concat([d_4,x1],dim=1)\n",
    "        \n",
    "        decoder_4 = self.conv9(decoder_4) # 1, 64, 64, 64\n",
    "        \n",
    "        output = self.out_conv(decoder_4)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet= UNET(num_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = unet(x)\n",
    "\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred.device\n",
    "\n",
    "# y.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = AdaptiveWingLoss(whetherWeighted=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4971\n",
    "# 1.1691"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainStep(model,trainLoader,optimizer,loss_fn):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    total_step = 0\n",
    "    \n",
    "    for _,(x,y) in enumerate(trainLoader):\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        \n",
    "        loss = loss_fn(y_pred,y)\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        total_step += 1\n",
    "        \n",
    "    return epoch_loss/len(trainLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valStep(model,testLoader,loss_fn):\n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "    total_val_mse_loss = 0\n",
    "    total_val_jaccard_index = 0\n",
    "    \n",
    "    total_step = 0\n",
    "    \n",
    "    for (x,y) in testLoader:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            y_pred = model(x)\n",
    "        \n",
    "    \n",
    "        loss = loss_fn(y_pred,y).item()\n",
    "        \n",
    "        total_val_mse_loss += loss\n",
    "        \n",
    "       \n",
    "        \n",
    "    return total_val_mse_loss/len(testLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model,trainLoader,testLoader,optimizer,loss_fn,epochs=100):\n",
    "    \n",
    "    \n",
    "    val_loss = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        trainLoss=trainStep(model,trainLoader,optimizer,loss_fn)\n",
    "        valLoss=valStep(model,testLoader,loss_fn)\n",
    "        \n",
    "        \n",
    "        if epoch==0:\n",
    "            val_loss = valLoss\n",
    "            \n",
    "        elif val_loss<valLoss  and abs(val_loss-valLoss) > 0.2:\n",
    "            \n",
    "            model_name = f\"hm_model_{str(val_loss)}.pth\"\n",
    "            torch.save(model, model_name)\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        print(f\"Epoch {epoch+1}| Train MSE Loss--> {train_epoch_mse_loss}\")\n",
    "        print(f\"Epoch {epoch+1}| VAL MSE Loss--> {val_mse}\")\n",
    "        print(f\"Epoch {epoch+1}| VAL Jaccard Loss--> {val_jaccard}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNET(num_classes=1)\n",
    "parameters = filter(lambda p: p.requires_grad,model.parameters())\n",
    "\n",
    "optimizer = torch.optim.Adam(parameters,lr=0.006)\n",
    "\n",
    "main(model,trainLoader,testLoader,optimizer,criterion,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AddCoords(nn.Module):\n",
    "\n",
    "    def __init__(self, with_r=False, with_boundary=False):\n",
    "        super().__init__()\n",
    "        self.with_r = with_r\n",
    "        self.with_boundary = with_boundary\n",
    "\n",
    "    def forward(self, input_tensor, boundary_map):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_tensor: shape(batch, channel, x_dim, y_dim)\n",
    "        \"\"\"\n",
    "        batch_size, _, x_dim, y_dim = input_tensor.size()\n",
    "\n",
    "        xx_channel = torch.arange(x_dim).repeat(1, y_dim, 1)\n",
    "        yy_channel = torch.arange(y_dim).repeat(1, x_dim, 1).transpose(1, 2)\n",
    "\n",
    "        xx_channel = xx_channel.float() / (x_dim - 1)\n",
    "        yy_channel = yy_channel.float() / (y_dim - 1)\n",
    "\n",
    "        xx_channel = xx_channel * 2 - 1\n",
    "        yy_channel = yy_channel * 2 - 1\n",
    "\n",
    "        xx_channel = xx_channel.repeat(batch_size, 1, 1, 1).transpose(2, 3).cuda()\n",
    "        yy_channel = yy_channel.repeat(batch_size, 1, 1, 1).transpose(2, 3).cuda()\n",
    "\n",
    "        ret = torch.cat([\n",
    "            input_tensor,\n",
    "            xx_channel.type_as(input_tensor),\n",
    "            yy_channel.type_as(input_tensor)], dim=1)\n",
    "\n",
    "        if self.with_r:\n",
    "            rr = torch.sqrt(torch.pow(xx_channel.type_as(input_tensor) - 0.5, 2) + torch.pow(yy_channel.type_as(input_tensor) - 0.5, 2))\n",
    "            ret = torch.cat([ret, rr], dim=1)\n",
    "\n",
    "        if self.with_boundary and (boundary_map is not None):\n",
    "            # B, 64(H), 64(W)\n",
    "            boundary_map = boundary_map.view(boundary_map.shape[0],1,boundary_map.shape[1],boundary_map.shape[2])\n",
    "            boundary_channel = torch.clamp(boundary_map,0.0, 1.0)\n",
    "            zero_tensor = torch.zeros_like(xx_channel)\n",
    "            xx_boundary_channel = torch.where(boundary_channel>0.05, xx_channel, zero_tensor)\n",
    "            yy_boundary_channel = torch.where(boundary_channel>0.05, yy_channel, zero_tensor)\n",
    "\n",
    "            ret = torch.cat([ret, xx_boundary_channel, yy_boundary_channel], dim=1)\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n",
    "class CoordConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, with_r=True, with_boundary=False,**kwargs):\n",
    "        super().__init__()\n",
    "        self.addcoords = AddCoords(with_r=with_r,with_boundary=with_boundary)\n",
    "        in_size = in_channels+2\n",
    "        if with_r:\n",
    "            in_size += 1\n",
    "        if with_boundary:\n",
    "            in_size += 2\n",
    "        self.conv = nn.Conv2d(in_size, out_channels, **kwargs)\n",
    "\n",
    "    def forward(self, x, boundary_map=None):\n",
    "        ret = self.addcoords(x, boundary_map)\n",
    "        ret = self.conv(ret)\n",
    "        return ret\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " x = torch.randn(1,3,633,357)\n",
    "coordconv1 = CoordConv(3,64,kernel_size=3,padding=1)\n",
    "out = coordconv1(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "\n",
    "__all__ = ['MobileNetV2', 'mobilenetv2']\n",
    "\n",
    "\n",
    "DEBUG=False\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" \n",
    "    Bottleneck Residual Block\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, expansion=1, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "        if expansion == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, in_channels, 3, stride, 1, groups=in_channels, bias=False),\n",
    "                nn.BatchNorm2d(in_channels),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "        else:\n",
    "            channels = expansion * in_channels\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, channels, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(channels),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                nn.Conv2d(channels, channels, 3, stride, 1, groups=channels, bias=False),\n",
    "                nn.BatchNorm2d(channels),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                nn.Conv2d(channels, out_channels, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "        self.residual = (stride == 1) and (in_channels == out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        if self.residual:\n",
    "            out = out + x\n",
    "        return out\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        in_channels = config[0][1]\n",
    "        features = [nn.Sequential(\n",
    "            nn.Conv2d(3, in_channels, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )]\n",
    "        for expansion, out_channels, blocks, stride in config[1:]:\n",
    "            for i in range(blocks):\n",
    "                features.append(Block(in_channels, out_channels, expansion, stride if i == 0 else 1))\n",
    "                in_channels = out_channels\n",
    "        self.features = nn.Sequential(*features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c2 = self.features[:4](x)\n",
    "        c3 = self.features[4:7](c2)\n",
    "        c4 = self.features[7:14](c3)\n",
    "        kwargs = {'size': c2.shape[-2:],'mode': 'bilinear','align_corners': False}\n",
    "        features =  torch.cat([F.interpolate(xx,**kwargs) for xx in [c2,c3,c4]], 1)\n",
    "\n",
    "        if DEBUG:\n",
    "            print(f'------------------------- \\nFeatures shape mobilev2: {features.shape}\\n---------------------------------')\n",
    "\n",
    "        return features\n",
    "\n",
    "def mobilenetv2(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a MobileNetv2 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    config = [\n",
    "        (1,  32, 1, 1),\n",
    "        (1,  16, 1, 1),\n",
    "        (6,  24, 2, 2), \n",
    "        (6,  32, 3, 2),\n",
    "        (6,  64, 4, 2),\n",
    "        (6,  96, 3, 1),\n",
    "    ]\n",
    "    model = MobileNetV2(config=config)\n",
    "    if pretrained:\n",
    "        assert kwargs[\"model_url\"] is not None, f'Model url should not be  None'\n",
    "        model.load_state_dict(model_zoo.load_url(kwargs[\"model_url\"]), strict=False)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((1, 3, 64,64))\n",
    "model = mobilenetv2()\n",
    "a = model(x)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap2coord(heatmap, topk=7):\n",
    "    N, C, H, W = heatmap.shape\n",
    "    score, index = heatmap.view(N,C,1,-1).topk(topk, dim=-1)\n",
    "    coord = torch.cat([index%W, index//W], dim=2)\n",
    "    return (coord*F.softmax(score, dim=-1)).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap2coord(heatmap, topk=7):\n",
    "    N, C, H, W = heatmap.shape\n",
    "    score, index = heatmap.view(N,C,1,-1).topk(topk, dim=-1)\n",
    "    coord = torch.cat([index%W, index//W], dim=2)\n",
    "    return (coord*F.softmax(score, dim=-1)).sum(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\\ Predicted heatmap to topk softmax heatmap\n",
    " Used when training model. After the decode step, we ave the heatmap \n",
    " then we get only topk points in that and get softmax of those\n",
    "\"\"\"\n",
    "def heatmap2topkheatmap(heatmap, topk=7):\n",
    "    \"\"\"\n",
    "    \\ Find topk value in each heatmap and calculate softmax for them.\n",
    "    \\ Another non topk points will be zero.\n",
    "    \\Based on that https://discuss.pytorch.org/t/how-to-keep-only-top-k-percent-values/83706\n",
    "    \"\"\"\n",
    "    N, C, H, W = heatmap.shape\n",
    "   \n",
    "    # Get topk points in each heatmap\n",
    "    # And using softmax for those score\n",
    "    heatmap = heatmap.view(N,C,1,-1)\n",
    "    \n",
    "    score, index = heatmap.topk(topk, dim=-1)\n",
    "    score = F.softmax(score, dim=-1)\n",
    "    heatmap = F.softmax(heatmap, dim=-1)\n",
    "\n",
    "\n",
    "    # Assign non-topk zero values\n",
    "    # Assign topk with calculated softmax value\n",
    "    res = torch.zeros(heatmap.shape)\n",
    "    res = res.scatter(-1, index, score)\n",
    "\n",
    "    # Reshape to the original size\n",
    "    heatmap = res.view(N, C, H, W)\n",
    "    # heatmap = heatmap.view(N, C, H, W)\n",
    "\n",
    "\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_topk_activation(heatmap, topk=7):\n",
    "    \"\"\"\n",
    "    \\ Find topk value in each heatmap and calculate softmax for them.\n",
    "    \\ Another non topk points will be zero.\n",
    "    \\Based on that https://discuss.pytorch.org/t/how-to-keep-only-top-k-percent-values/83706\n",
    "    \"\"\"\n",
    "    N, C, H, W = heatmap.shape\n",
    "   \n",
    "    # Get topk points in each heatmap\n",
    "    # And using softmax for those score\n",
    "    heatmap = heatmap.view(N,C,1,-1)\n",
    "    \n",
    "    score, index = heatmap.topk(topk, dim=-1)\n",
    "    score = F.sigmoid(score)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap2softmaxheatmap(heatmap):\n",
    "    N, C, H, W = heatmap.shape\n",
    "   \n",
    "    # Get topk points in each heatmap\n",
    "    # And using softmax for those score\n",
    "    heatmap = heatmap.view(N,C,1,-1)\n",
    "    heatmap = F.softmax(heatmap, dim=-1)\n",
    "\n",
    "\n",
    "    # Reshape to the original size\n",
    "    heatmap = heatmap.view(N, C, H, W)\n",
    "\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap2sigmoidheatmap(heatmap):\n",
    "    heatmap = F.sigmoid(heatmap)\n",
    "\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaussian(t, x, y, sigma=10):\n",
    "    \"\"\"\n",
    "    Generates a 2D Gaussian point at location x,y in tensor t.\n",
    "    \n",
    "    x should be in range (-1, 1) to match the output of fastai's PointScaler.\n",
    "    \n",
    "    sigma is the standard deviation of the generated 2D Gaussian.\n",
    "    \"\"\"\n",
    "    _gaussians = {}\n",
    "\n",
    "\n",
    "    h,w = t.shape\n",
    "    \n",
    "    # Heatmap pixel per output pixel\n",
    "    mu_x = int(0.5 * (x + 1.) * w)\n",
    "    mu_y = int(0.5 * (y + 1.) * h)\n",
    "    \n",
    "    tmp_size = sigma * 3\n",
    "    \n",
    "    # Top-left\n",
    "    x1,y1 = int(mu_x - tmp_size), int(mu_y - tmp_size)\n",
    "    \n",
    "    # Bottom right\n",
    "    x2, y2 = int(mu_x + tmp_size + 1), int(mu_y + tmp_size + 1)\n",
    "    if x1 >= w or y1 >= h or x2 < 0 or y2 < 0:\n",
    "        return t\n",
    "    \n",
    "    size = 2 * tmp_size + 1\n",
    "    tx = np.arange(0, size, 1, np.float32)\n",
    "    ty = tx[:, np.newaxis]\n",
    "    x0 = y0 = size // 2\n",
    "    \n",
    "    # The gaussian is not normalized, we want the center value to equal 1\n",
    "    g = _gaussians[sigma] if sigma in _gaussians \\\n",
    "                else torch.Tensor(np.exp(- ((tx - x0) ** 2 + (ty - y0) ** 2) / (2 * sigma ** 2)))\n",
    "    _gaussians[sigma] = g\n",
    "    \n",
    "    # Determine the bounds of the source gaussian\n",
    "    g_x_min, g_x_max = max(0, -x1), min(x2, w) - x1\n",
    "    g_y_min, g_y_max = max(0, -y1), min(y2, h) - y1\n",
    "    \n",
    "    # Image range\n",
    "    img_x_min, img_x_max = max(0, x1), min(x2, w)\n",
    "    img_y_min, img_y_max = max(0, y1), min(y2, h)\n",
    "    \n",
    "    t[img_y_min:img_y_max, img_x_min:img_x_max] = \\\n",
    "      g[g_y_min:g_y_max, g_x_min:g_x_max]\n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coord2heatmap(w, h, ow, oh, x, y, random_round=False, random_round_with_gaussian=False):\n",
    "    \"\"\"\n",
    "    Inserts a coordinate (x,y) from a picture with \n",
    "    original size (w x h) into a heatmap, by randomly assigning \n",
    "    it to one of its nearest neighbor coordinates, with a probability\n",
    "    proportional to the coordinate error.\n",
    "    \n",
    "    Arguments:\n",
    "    x: x coordinate\n",
    "    y: y coordinate\n",
    "    w: original width of picture with x coordinate\n",
    "    h: original height of picture with y coordinate\n",
    "    \"\"\"\n",
    "    # Get scale\n",
    "    sx = ow / w\n",
    "    sy = oh / h\n",
    "    \n",
    "    # Unrounded target points\n",
    "    px = x * sx\n",
    "    py = y * sy\n",
    "    \n",
    "    # Truncated coordinates\n",
    "    nx,ny = int(px), int(py)\n",
    "    \n",
    "    # Coordinate error\n",
    "    ex,ey = px - nx, py - ny\n",
    "\n",
    "    # Heatmap    \n",
    "    heatmap = torch.zeros(ow, oh)\n",
    "\n",
    "    if random_round_with_gaussian:\n",
    "        xyr = torch.rand(2)\n",
    "        xx = (ex >= xyr[0]).long()\n",
    "        yy = (ey >= xyr[1]).long()\n",
    "        row = min(ny + yy, heatmap.shape[0] - 1)\n",
    "        col = min(nx+xx, heatmap.shape[1] - 1)\n",
    "\n",
    "        # Normalize into - 1, 2\n",
    "        col = (col/float(ow)) * (2) + (-1)\n",
    "        row = (row/float(oh)) * (2) + (-1)\n",
    "        heatmap = generate_gaussian(heatmap, col, row, sigma=1.5)\n",
    "\n",
    "\n",
    "    elif random_round:\n",
    "        xyr = torch.rand(2)\n",
    "        xx = (ex >= xyr[0]).long()\n",
    "        yy = (ey >= xyr[1]).long()\n",
    "        heatmap[min(ny + yy, heatmap.shape[0] - 1), \n",
    "                min(nx+xx, heatmap.shape[1] - 1)] = 1\n",
    "    else:\n",
    "        nx = min(nx, ow-1)\n",
    "        ny = min(ny, oh-1)\n",
    "        heatmap[ny][nx] = (1-ex) * (1-ey)\n",
    "        if (ny+1<oh-1):\n",
    "            heatmap[ny+1][nx] = (1-ex) * ey\n",
    "        \n",
    "        if (nx+1<ow-1):\n",
    "            heatmap[ny][nx+1] = ex * (1-ey)\n",
    "        \n",
    "        if (nx+1<ow-1 and ny+1<oh-1):\n",
    "            heatmap[ny+1][nx+1] = ex * ey\n",
    "    \n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lmks2heatmap(lmks, random_round=False, random_round_with_gaussian=False):\n",
    "    w,h,ow,oh=256,256,64,64\n",
    "    heatmap = torch.rand((lmks.shape[0],lmks.shape[1], ow, oh))\n",
    "    for i in range(lmks.shape[0]):  # num_lmks\n",
    "        for j in range(lmks.shape[1]):\n",
    "            heatmap[i][j] = coord2heatmap(w, h, ow, oh, lmks[i][j][0], lmks[i][j][1], random_round=random_round, random_round_with_gaussian=random_round_with_gaussian)\n",
    "    \n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
