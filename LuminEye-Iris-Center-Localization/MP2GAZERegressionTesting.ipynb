{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "from basicsr.utils.download_util import load_file_from_url\n",
    "from torchvision import transforms  \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import albumentations as A\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "from imutils import face_utils\n",
    "from scipy.spatial import distance as dist\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe\n",
    "from BaseModels.resnetModels import BB_model\n",
    "from BaseModels.efficientnetModels import BB_model\n",
    "import pandas as pd\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from scipy.spatial import distance\n",
    "# device = \"cpu\"\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    \"\"\"Load Regression model\n",
    "\n",
    "    Args:\n",
    "        model_path (_str_): _model path_\n",
    "    \n",
    "\n",
    "    Returns:\n",
    "        _torch model_: _RESNET model_\n",
    "    \"\"\"\n",
    "\n",
    "    model = torch.load(model_path,map_location=device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# Latest Model\n",
    "# regression_model_path = \"/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Iris-Segmentation/Regression_model_1.140251636505127.pth\"\n",
    "\n",
    "# Mix Dataset\n",
    "# regression_model_path = '/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Iris-Center-Localization/Regression_model_1.487574208665777.pth'\n",
    "\n",
    "# regression_model_path = \"/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Iris-Center-Localization/RecentModels/SpheroPipeLine/Resnet_32_IMG_SIZE__32/Regression_model_0.25639680131442016.pth\"\n",
    "\n",
    "# regression_model_path = \"/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Iris-Center-Localization/Trained_Models/Resnet_32_IMG_SIZE__32/Regression_model_0.25639680131442016.pth\"\n",
    "\n",
    "# regression_model_path = \"RecentModels/BioGi4eH2head/Regression_model_0.7620949026704394.pth\"\n",
    "\n",
    "# regression_model_path = \"RecentModels/SpheroPipeLine/Resnet_32_IMG_SIZE__32/Regression_model_0.25639680131442016.pth\"\n",
    "\n",
    "# regression_model_path = \"/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Iris-Center-Localization/RecentModelsonGpu/RecentModels/AllDataset/Regression_model_1.4074174204430023.pth\"\n",
    "\n",
    "# regression_model_path = \"/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Iris-Center-Localization/RecentModelsonGpu/Recent_EffcientNetModels/EfficientNet1_6_smoothl1loss_batch_64.pt\"\n",
    "\n",
    "\n",
    "# regression_model_path = \"/home/nipun/Documents/Uni_Malta/LuminEye/LuminEye-Iris-Center-Localization/RecentModelsonGpu/Recent_EffcientNetModels/EfficientNet1_6_smoothl1loss_batch_256.pt\"\n",
    "\n",
    "\n",
    "# regression_model_path = \"RecentModelsonGpu/Recent_EffcientNetModels/Regression_EffcientNet__epoch_500_SmoothL1Loss_summation_batch_256_resize_64_for_gi4e_bioid_h2head_1.52.pt\"\n",
    "\n",
    "# regression_model_path = \"RecentModelsonGpu/Recent_EffcientNetModels/EfficientNetWingLossV1.pt\"\n",
    "\n",
    "# regression_model_path = 'ResnetBaseWithMSEMEANwithoughtCoordConv/checkpoint.pt'\n",
    "\n",
    "\n",
    "regression_model_path = 'EffcienientNetNOTCoordconv.pt'\n",
    "\n",
    "REGRESSION_MODEL = load_model(\n",
    "    model_path=regression_model_path)\n",
    "\n",
    "\n",
    "\n",
    "RESIZE_AMT = 64\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.efficientnet import efficientnet_b3\n",
    "\n",
    "class BB_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        efficientnet = efficientnet_b3(pretrained=True)\n",
    "        \n",
    "        layers = list(efficientnet.children())[:1]\n",
    "        self.features1 = nn.Sequential(*layers)\n",
    "\n",
    "    \n",
    "        self.bb = nn.Sequential(nn.BatchNorm1d(1536),nn.Linear(1536,512),nn.ReLU(inplace=True),\n",
    "                                nn.BatchNorm1d(512),nn.Linear(512,2))\n",
    "        # self.coordConv =CoordConv(in_channels=3,out_channels=3,kernel_size=5,padding=0)\n",
    "\n",
    "    def freze(self):\n",
    "        \n",
    "        for param in self.features1.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    def forward(self,x):\n",
    "\n",
    "        # x = self.coordConv(x)\n",
    "        x = self.features1(x) #[1, 1536, 8, 8]\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        \n",
    "        x = nn.AdaptiveAvgPool2d((1,1))(x) # [ 1,1536,1,1]\n",
    "        \n",
    "        \n",
    "        x = x.view(x.shape[0],-1) # [1,1536]\n",
    "\n",
    "        \n",
    "        \n",
    "        return self.bb(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddCoords(nn.Module):\n",
    "\n",
    "    def __init__(self, with_r=False):\n",
    "        super().__init__()\n",
    "        self.with_r = with_r\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_tensor: shape(batch, channel, x_dim, y_dim)\n",
    "        \"\"\"\n",
    "        batch_size, _, x_dim, y_dim = input_tensor.size()\n",
    "\n",
    "        xx_channel = torch.arange(x_dim).repeat(1, y_dim, 1)\n",
    "        yy_channel = torch.arange(y_dim).repeat(1, x_dim, 1).transpose(1, 2)\n",
    "\n",
    "        xx_channel = xx_channel.float() / (x_dim - 1)\n",
    "        yy_channel = yy_channel.float() / (y_dim - 1)\n",
    "\n",
    "        xx_channel = xx_channel * 2 - 1\n",
    "        yy_channel = yy_channel * 2 - 1\n",
    "\n",
    "        xx_channel = xx_channel.repeat(batch_size, 1, 1, 1).transpose(2, 3)\n",
    "        yy_channel = yy_channel.repeat(batch_size, 1, 1, 1).transpose(2, 3)\n",
    "\n",
    "        ret = torch.cat([\n",
    "            input_tensor,\n",
    "            xx_channel.type_as(input_tensor),\n",
    "            yy_channel.type_as(input_tensor)], dim=1)\n",
    "\n",
    "        if self.with_r:\n",
    "            rr = torch.sqrt(torch.pow(xx_channel.type_as(input_tensor) - 0.5, 2) + torch.pow(yy_channel.type_as(input_tensor) - 0.5, 2))\n",
    "            ret = torch.cat([ret, rr], dim=1)\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n",
    "class CoordConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels,out_channels, with_r=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.addcoords = AddCoords(with_r=with_r)\n",
    "\n",
    "\n",
    "        \n",
    "        in_size = in_channels+2\n",
    "        if with_r:\n",
    "            in_size += 1\n",
    "        self.conv = nn.Conv2d(in_size, out_channels, **kwargs)\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ret = self.addcoords(x)\n",
    "        ret = self.conv(ret)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BioID\n",
    "val_csv_path = \"/home/nipun/Documents/Uni_Malta/Datasets/CenterRegression/MP2GAZE/AllCoordinatesMp2GazeTest.csv\"\n",
    "\n",
    "\n",
    "val_df = pd.read_csv(val_csv_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageName</th>\n",
       "      <th>Coordinates</th>\n",
       "      <th>DataSet Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MPIIGaze/Data/Original/p00/day02/0350.jpg\"</td>\n",
       "      <td>[576.75131084363, 446.9430824441167, 595.62107...</td>\n",
       "      <td>MP2GAZE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MPIIGaze/Data/Original/p00/day23/0201.jpg\"</td>\n",
       "      <td>[629.1027244977723, 418.5422721626321, 649.159...</td>\n",
       "      <td>MP2GAZE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MPIIGaze/Data/Original/p00/day22/0437.jpg\"</td>\n",
       "      <td>[519.3837103984533, 379.3424773235636, 551.963...</td>\n",
       "      <td>MP2GAZE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MPIIGaze/Data/Original/p00/day34/0296.jpg\"</td>\n",
       "      <td>[563.4499942709884, 374.6517620194357, 591.877...</td>\n",
       "      <td>MP2GAZE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MPIIGaze/Data/Original/p00/day10/0198.jpg\"</td>\n",
       "      <td>[659.4630047049818, 463.58724173032533, 685.19...</td>\n",
       "      <td>MP2GAZE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    ImageName  \\\n",
       "0  MPIIGaze/Data/Original/p00/day02/0350.jpg\"   \n",
       "1  MPIIGaze/Data/Original/p00/day23/0201.jpg\"   \n",
       "2  MPIIGaze/Data/Original/p00/day22/0437.jpg\"   \n",
       "3  MPIIGaze/Data/Original/p00/day34/0296.jpg\"   \n",
       "4  MPIIGaze/Data/Original/p00/day10/0198.jpg\"   \n",
       "\n",
       "                                         Coordinates DataSet Type  \n",
       "0  [576.75131084363, 446.9430824441167, 595.62107...      MP2GAZE  \n",
       "1  [629.1027244977723, 418.5422721626321, 649.159...      MP2GAZE  \n",
       "2  [519.3837103984533, 379.3424773235636, 551.963...      MP2GAZE  \n",
       "3  [563.4499942709884, 374.6517620194357, 591.877...      MP2GAZE  \n",
       "4  [659.4630047049818, 463.58724173032533, 685.19...      MP2GAZE  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectImgDir(datasetType:str):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if datasetType == \"MP2GAZE\":\n",
    "        return \"/home/nipun/Documents/Uni_Malta/Datasets/\"\n",
    "        \n",
    "        \n",
    "        \n",
    "    elif datasetType ==\"i2head\":\n",
    "        return \"/home/nipun/Documents/Uni_Malta/Datasets/\"\n",
    "        \n",
    "    elif datasetType == \"GI4E\":\n",
    "        \n",
    "        return \"/home/nipun/Documents/Uni_Malta/Datasets/gi4e_database/images/\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_image(model,image):\n",
    "        \n",
    "        val_transforms =  A.Compose([\n",
    "                                        A.Resize(width=RESIZE_AMT,height=RESIZE_AMT),\n",
    "                                        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "                                        ToTensorV2(p=1)\n",
    "                                        ])\n",
    "        \n",
    "        unnorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        transformed_img = val_transforms(image=image[:,:,::-1])\n",
    "        image = transformed_img['image']\n",
    "        \n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "                out_coord = model(image)\n",
    "        \n",
    "        \n",
    "        image = image.squeeze(0)\n",
    "\n",
    "        image = transforms.ToPILImage()(unnorm(image))\n",
    "        \n",
    "        \n",
    "        pred_coord = out_coord.detach().cpu().numpy()[0]\n",
    "        \n",
    "        return image,pred_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cropeyes(image, inner_array, outer_array):\n",
    "\n",
    "    arr = {\"top_left\": [inner_array[0]-5, inner_array[1]-20],\n",
    "           \"bottom_right\": [outer_array[0]+5, outer_array[1]+20]}\n",
    "\n",
    "    return image[arr[\"top_left\"][1]:arr[\"bottom_right\"][1], arr[\"top_left\"][0]:arr[\"bottom_right\"][0]], arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def rescale_coordinate(coord,original_image,resize_amt):\n",
    "    \n",
    "    h,w = original_image.shape[:2]\n",
    "    coord[0] = int((coord[0]/resize_amt) * w)\n",
    "    coord[1] = int((coord[1]/resize_amt) * h)\n",
    "    \n",
    "    return coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleCoorinatesToOriginalImage(pred_coords,eye_margin):\n",
    "    \n",
    "    # {'top_left': array([385, 214]), 'bottom_right': array([426, 226])}\n",
    "    \n",
    "    x1 = eye_margin[\"top_left\"][0] + pred_coords[0]\n",
    "    y1 = eye_margin[\"top_left\"][1] + pred_coords[1]\n",
    "    \n",
    "    return [x1,y1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateEuclideanDistance(coord1,coord2):\n",
    "    return (((coord1[0])-float(coord2[0]))^2  + (float(coord1[0])-float(coord2[0]))^2) ^ 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = \"/home/nipun/Documents/Uni_Malta/Datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02255384108019632\n",
      "0.016765219143761074\n",
      "0.0\n",
      "0.026126497213658204\n",
      "0.03217643201321576\n",
      "0.01563339910144465\n",
      "0.01248362597931077\n",
      "0.01288890976575717\n",
      "0.008163605321754816\n",
      "0.026572823627058487\n",
      "0.02046953170891141\n",
      "0.02049584873004486\n",
      "0.006112464870141717\n",
      "0.01945768347613674\n",
      "0.015514787986130315\n",
      "0.020461495722482567\n",
      "0.03243574962539294\n",
      "0.020643253864456434\n",
      "0.014517167628488535\n",
      "0.02388410885962159\n",
      "0.027700701129007948\n",
      "0.011145218132094787\n",
      "0.01642569885854426\n",
      "0.014063042481340066\n",
      "0.023596458909150436\n",
      "0.01817280910462035\n",
      "0.028447458770443367\n",
      "0.032584421552822575\n",
      "0.02343312028655591\n",
      "0.01552787542996996\n",
      "0.011974115989261225\n",
      "0.007574888148133693\n",
      "0.05233820578865744\n",
      "0.007983666160462774\n",
      "0.015540573797716228\n",
      "0.01208244186660354\n",
      "0.011896334946882258\n",
      "0.007467574778976377\n",
      "0.028239787090084047\n",
      "0.01492371097349751\n",
      "0.029411255865026874\n",
      "0.018773922658852197\n",
      "0.015037168948034583\n",
      "0.019611613513818404\n",
      "0.010959932487023821\n",
      "0.011497671238805652\n",
      "0.01552824984374854\n",
      "0.018469811292500967\n",
      "0.01049785603183589\n",
      "0.006982089475309836\n",
      "0.021502752229770942\n",
      "0.00980203746722279\n",
      "0.021966088569534453\n",
      "0.008333043996551019\n",
      "0.03526559380716956\n",
      "0.03098932137026761\n",
      "0.03789945724575915\n",
      "0.017192386421410154\n",
      "0.01341949470380791\n",
      "0.05234239225902137\n",
      "0.02678357920027901\n",
      "0.012397752739056737\n",
      "0.007462686567164179\n",
      "0.05547001962252292\n",
      "0.0187666481231423\n",
      "0.02129202335170726\n",
      "0.031597266031154904\n",
      "0.029781828234786756\n",
      "0.033481372551702956\n",
      "0.03648766040369611\n",
      "0.03866570158279926\n",
      "0.006288810682967743\n",
      "0.03355704697986577\n",
      "0.0180150939644418\n",
      "0.02281227009624893\n",
      "0.027983662310423867\n",
      "0.053474721475082775\n",
      "0.020894181926626287\n",
      "0.04449941594899848\n",
      "0.045610791846394126\n",
      "0.02108653748577924\n",
      "0.023968860708479194\n",
      "0.053008653589502734\n",
      "0.023516721041961276\n",
      "0.023260846590195953\n",
      "0.051528757092492265\n",
      "0.016765219143761074\n",
      "0.042974297882524784\n",
      "0.02338852139158211\n",
      "0.006356545677998633\n",
      "0.009751110166144314\n",
      "0.017192386421410154\n",
      "0.034910247158340076\n",
      "0.03226645958836384\n",
      "0.02128044950231881\n",
      "0.01927358361229249\n",
      "0.05080404812870497\n",
      "0.027728345144624023\n",
      "0.025084804565425216\n",
      "0.0424065130967231\n",
      "0.019218690270130137\n",
      "0.047062784839523066\n",
      "0.030678599553894816\n",
      "0.030406164562583538\n",
      "0.025099022738377084\n",
      "0.032009219983223994\n",
      "0.026352313834736494\n",
      "0.029953010631053222\n",
      "0.012456285080792119\n",
      "0.026299402984029215\n",
      "0.011401611731735473\n",
      "0.027025930306713835\n",
      "0.03392999828208182\n",
      "0.019105412604086193\n",
      "0.013094570021973104\n",
      "0.02194426104723437\n",
      "0.025675748612326747\n",
      "0.030396505818525113\n",
      "0.014104666976770626\n",
      "0.020672455764868078\n",
      "0.015868518630859304\n",
      "0.03697466539056922\n",
      "0.018018018018018018\n",
      "0.014865882924943325\n",
      "0.01974534749528259\n",
      "0.02556549962824568\n",
      "0.008527098476033985\n",
      "0.019739576307964544\n"
     ]
    }
   ],
   "source": [
    "maximizedNormalizedError = []\n",
    "for i, (_, rows) in enumerate(val_df.iterrows()):\n",
    "\n",
    "        bbox = [int(float(x)) for x in rows[\"Coordinates\"][1:-1].split(',')]\n",
    "\n",
    "        left_inner = bbox[0:2]\n",
    "        left_center = bbox[2:4]\n",
    "        left_outer = bbox[4:6]\n",
    "\n",
    "        right_inner = bbox[6:8]\n",
    "        right_center = bbox[8:10]\n",
    "        right_outer = bbox[10:12]\n",
    "\n",
    "        img_path = os.path.join(IMG_DIR, rows[\"ImageName\"][:-1])\n",
    "        # #\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "\n",
    "        IPD = distance.euclidean(left_center, right_center)\n",
    "        \n",
    "\n",
    "       \n",
    "\n",
    "        left_eye, Leye = cropeyes(img, left_inner, left_outer)\n",
    "        right_eye, Reye = cropeyes(img, right_inner, right_outer)\n",
    "        \n",
    "        \n",
    "        _, pred_l_eye = prediction_image(\n",
    "            model=REGRESSION_MODEL, image=left_eye)\n",
    "\n",
    "        _, pred_r_eye = prediction_image(\n",
    "            model=REGRESSION_MODEL, image=right_eye)\n",
    "        \n",
    "        \n",
    "        pred_l_eye = rescale_coordinate(pred_l_eye, left_eye, RESIZE_AMT)\n",
    "\n",
    "        pred_r_eye = rescale_coordinate(pred_r_eye, right_eye, RESIZE_AMT)\n",
    "        \n",
    "        \n",
    "        pred_l_eye_toOriginaImage = scaleCoorinatesToOriginalImage(\n",
    "            pred_l_eye, Leye)\n",
    "        pred_r_eye_toOriginaImage = scaleCoorinatesToOriginalImage(\n",
    "            pred_r_eye, Reye)\n",
    "        \n",
    "        \n",
    "        cv2.circle(img, (int(pred_l_eye_toOriginaImage[0]), int(\n",
    "            pred_l_eye_toOriginaImage[1])), 1, (0, 255, 0), -1)\n",
    "\n",
    "        cv2.circle(img, (int(pred_r_eye_toOriginaImage[0]), int(\n",
    "            pred_r_eye_toOriginaImage[1])), 1, (0, 255, 0), -1)\n",
    "        \n",
    "        \n",
    "        cv2.circle(img, (int(left_inner[0]), int(\n",
    "            left_inner[1])), 1, (0, 0, 255), -1)\n",
    "        cv2.circle(img, (int(left_center[0]), int(\n",
    "            left_center[1])), 1, (0, 0, 255), -1)\n",
    "        cv2.circle(img, (int(left_outer[0]), int(\n",
    "            left_outer[1])), 1, (0, 0, 255), -1)\n",
    "\n",
    "        cv2.circle(img, (int(right_inner[0]), int(\n",
    "            right_inner[1])), 1, (0, 0, 255), -1)\n",
    "        cv2.circle(img, (int(right_center[0]), int(\n",
    "            right_center[1])), 1, (0, 0, 255), -1)\n",
    "        cv2.circle(img, (int(right_outer[0]), int(\n",
    "            right_outer[1])), 1, (0, 0, 255), -1)\n",
    "        \n",
    "         \n",
    "          \n",
    "        left_eye_euclidea_distance = distance.euclidean(\n",
    "            pred_l_eye_toOriginaImage, left_center)\n",
    "\n",
    "        right_eye_euclidea_distance = distance.euclidean(\n",
    "            pred_r_eye_toOriginaImage, right_center)\n",
    "        \n",
    "        eMax = max(left_eye_euclidea_distance, right_eye_euclidea_distance)/IPD\n",
    "\n",
    "        print(eMax)\n",
    "\n",
    "        maximizedNormalizedError.append(eMax)\n",
    "    \n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckForLess(list1, val):\n",
    "    \n",
    "    l1 = []\n",
    "    for x in list1:\n",
    "        \n",
    "        if x <= val:\n",
    "            l1.append(x)\n",
    "    return l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_0_25 = CheckForLess(maximizedNormalizedError,0.25)\n",
    "\n",
    "e_0_05 = CheckForLess(maximizedNormalizedError,0.05)\n",
    "\n",
    "e_0_1 = CheckForLess(maximizedNormalizedError,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(e_0_25)/len(maximizedNormalizedError)) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.53125"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(e_0_05)/len(maximizedNormalizedError)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(e_0_1)/len((maximizedNormalizedError)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(img[:,:,::-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(right_eye)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path =  \"/home/nipun/Documents/Uni_Malta/Datasets/columbia_gaze_data_set/Columbia Gaze Data Set/0039/0039_2m_0P_10V_5H.jpg\"\n",
    "\n",
    "frame = cv2.imread(img_path)\n",
    "\n",
    "plt.imshow(frame[:,:,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # MediaPipe\n",
    "shape_array = captureFaceLandmarks(frame)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_eye, right_eye,Leye,Reye = cropped_image(frame, shape_array)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(left_eye[:,:,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(right_eye[:,:,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left = \"/home/nipun/Documents/Uni_Malta/Datasets/Center_Regression/MP2GAZE/Images/417_left.png\"\n",
    "\n",
    "# left_eye = cv2.imread(left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,pred_l_eye = prediction_image(model=REGRESSION_MODEL,image=left_eye)\n",
    "        \n",
    "        \n",
    "_,pred_r_eye = prediction_image(model=REGRESSION_MODEL,image=right_eye)\n",
    "\n",
    "\n",
    "pred_l_eye = rescale_coordinate(pred_l_eye,left_eye,RESIZE_AMT)\n",
    "\n",
    "pred_r_eye = rescale_coordinate(pred_r_eye,right_eye,RESIZE_AMT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.circle(left_eye,(int(pred_l_eye[0]),int(pred_l_eye[1])),1,(0,255,0),-1)\n",
    "cv2.circle(right_eye,(int(pred_r_eye[0]),int(pred_r_eye[1])),1,(0,255,0),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(left_eye[:,:,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(right_eye[:,:,::-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoordConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.ion()\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    # generate all of the input images\n",
    "    n_samples = 56 ** 2\n",
    "    onehots = np.pad(np.eye(n_samples).reshape((n_samples, 1, 56, 56)), ((0,0), (0, 0), (4,4), (4,4)), \"constant\")\n",
    "    onehots = torch.from_numpy(onehots).float()\n",
    "    with torch.no_grad():\n",
    "        conv = nn.Conv2d(1, 1, kernel_size=9, padding=4, stride=1)\n",
    "        conv.weight.data.fill_(1)\n",
    "        conv.bias.data.fill_(0)\n",
    "        dataset_x = conv(onehots)\n",
    "    y_i = torch.arange(56).view(56, 1).repeat(1, 56).view(-1, 1)\n",
    "    y_j = torch.arange(56).repeat(56, 1).view(-1, 1)\n",
    "    dataset_y = torch.cat((y_i, y_j), dim=1)\n",
    "\n",
    "    # split the dataset tensor into train, test sets\n",
    "    sample_order = np.arange(n_samples)\n",
    "    \n",
    "    np.random.shuffle(sample_order)\n",
    "    train_idxes = sample_order[:2352]\n",
    "    test_idxes = sample_order[2352:]\n",
    "\n",
    "    train_x = dataset_x[train_idxes]\n",
    "    train_y = dataset_y[train_idxes] \n",
    "    test_x = dataset_x[test_idxes]\n",
    "    test_y = dataset_y[test_idxes]\n",
    "\n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 8**2\n",
    "x = np.pad(np.ones((64,64)).reshape(samples,1,8,8), ((2,2), (0, 0), (4,4), (4,4)),\"constant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[63]\n",
    "\n",
    "\n",
    "# array([[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "#         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "#         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "#         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "#         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "#         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "#         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "#         [0., 0., 0., 0., 0., 0., 0., 1.]]])\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.eye(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(8).view(8,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(8).view(8,1).repeat(1,56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(8).view(8,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(8).view(8,1).repeat(1,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.ion()\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "def generate_data():\n",
    "    # generate all of the input images\n",
    "    n_samples = 56 ** 2\n",
    "    onehots = np.pad(np.eye(n_samples).reshape((n_samples, 1, 56, 56)), ((0,0), (0, 0), (4,4), (4,4)), \"constant\")\n",
    "    onehots = torch.from_numpy(onehots).float()\n",
    "    with torch.no_grad():\n",
    "        conv = nn.Conv2d(1, 1, kernel_size=9, padding=4, stride=1)\n",
    "        conv.weight.data.fill_(1)\n",
    "        conv.bias.data.fill_(0)\n",
    "        dataset_x = conv(onehots)\n",
    "    y_i = torch.arange(56).view(56, 1).repeat(1, 56).view(-1, 1)\n",
    "    y_j = torch.arange(56).repeat(56, 1).view(-1, 1)\n",
    "    dataset_y = torch.cat((y_i, y_j), dim=1)\n",
    "\n",
    "    # split the dataset tensor into train, test sets\n",
    "    sample_order = np.arange(n_samples)\n",
    "    \n",
    "    np.random.shuffle(sample_order)\n",
    "    train_idxes = sample_order[:2352]\n",
    "    test_idxes = sample_order[2352:]\n",
    "\n",
    "    train_x = dataset_x[train_idxes]\n",
    "    train_y = dataset_y[train_idxes] \n",
    "    test_x = dataset_x[test_idxes]\n",
    "    test_y = dataset_y[test_idxes]\n",
    "\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "\n",
    "class CoordConv2d(nn.Module):\n",
    "    \n",
    "    def __init__(self, device, in_channels, out_channels, kernel_size, padding, stride, input_size):\n",
    "        super(CoordConv2d, self).__init__()\n",
    "        self.device = device\n",
    "        self.cc_xy = self.make_channels(input_size)\n",
    "        self.conv = nn.Conv2d(in_channels+2, out_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "\n",
    "    def make_channels(self, input_size):\n",
    "        coord_vals = (2 * torch.arange(input_size) / input_size) - 1\n",
    "        xchannel = coord_vals.repeat((input_size, 1)).unsqueeze(dim=0)\n",
    "        ychannel = xchannel.permute(0, 2, 1)\n",
    "        return torch.cat((xchannel.unsqueeze(dim=0), ychannel.unsqueeze(dim=0)), dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n = x.shape[0]\n",
    "        x = torch.cat((x, self.cc_xy.repeat(n, 1, 1, 1).to(self.device)), dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, device, coordconv=True):\n",
    "        super(CNN, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=5, padding=2, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=1)\n",
    "        self.bn2 = nn.BatchNorm2d(2)\n",
    "        self.bn3 = nn.BatchNorm2d(2)\n",
    "        # input (N, C, H, W) is (N, 1, 64, 64)\n",
    "        if coordconv:\n",
    "            self.conv1 = CoordConv2d(device, 1, 1, kernel_size=1, padding=0, stride=1, input_size=64)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(1, 1, kernel_size=1, padding=0, stride=1)\n",
    "        self.conv2 = nn.Conv2d(1, 2, kernel_size=3, padding=1, stride=2)\n",
    "        self.conv3 = nn.Conv2d(2, 2, kernel_size=3, padding=1, stride=2)\n",
    "        self.fc1 = nn.Linear(2 * 4 * 4, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.conv1(x))\n",
    "        x = self.relu(self.pool(self.bn2(self.conv2(x))))\n",
    "        x = self.relu(self.pool(self.bn3(self.conv3(x))))\n",
    "        x = self.relu(self.fc1(x.view(x.size(0), -1)))\n",
    "        return x\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "train_x, train_y, test_x, test_y = generate_data()\n",
    "train_x, train_y, test_x, test_y = train_x.to(device), train_y.to(device), test_x.to(device), test_y.to(device)\n",
    "models = [CNN(device=device, coordconv=False).to(device), CNN(device=device, coordconv=True).to(device)]\n",
    "model_names = [\"Standard\", \"CoordConv\"]\n",
    "n_epochs = 5000\n",
    "batch_size = 32\n",
    "\n",
    "loss_history = [[],[]]  # append validation losses as the models train for eventual plotting.\n",
    "\n",
    "for model_idx, model in enumerate(models):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "    criterion = nn.L1Loss()\n",
    "    for epoch_i in range(n_epochs):\n",
    "        model.train()\n",
    "        # sample a batch\n",
    "        sample_idxes = np.random.randint(2352, size=(batch_size))\n",
    "        pred_y = model(train_x[sample_idxes])\n",
    "        loss = criterion(pred_y, train_y[sample_idxes])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch_i + 1) % 50 == 0:\n",
    "            # evaluate the model\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                test_loss = criterion(model(test_x), test_y)\n",
    "            print(\"Epoch: {}/{}, Model: {}, Train Loss: {:.3f}, Test Loss: {:.3f}\".format(epoch_i + 1, n_epochs, model_names[model_idx], loss.item(), test_loss.item()))\n",
    "            loss_history[model_idx].append(test_loss.item())\n",
    "\n",
    "    print(\"---\" * 10, \"\\n\")\n",
    "\n",
    "\n",
    "# plot the validation loss histories\n",
    "plt.ion()\n",
    "epoch_x = np.arange(50, n_epochs+1, 50)\n",
    "plt.plot(epoch_x, loss_history[0], label=\"Standard\", lw=3)\n",
    "plt.plot(epoch_x, loss_history[1], label=\"CoordConv\", lw=3)\n",
    "plt.legend()\n",
    "plt.title(\"Supervised Regression Error\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Error\")\n",
    "\n",
    "\n",
    "# Functions to show the CoordConv predictions on input images after training, if desired.\n",
    "def test_show(sample_idx):\n",
    "    plt.imshow(test_x[sample_idx, 0].detach().cpu().numpy(), cmap='Reds')\n",
    "    with torch.no_grad():\n",
    "        pred_y = model(test_x)[sample_idx]\n",
    "    print(\"Predicted: {}, Target: {}\".format(pred_y.detach().cpu().numpy(), test_y[sample_idx].detach().cpu().numpy()))\n",
    "\n",
    "def train_show(sample_idx):\n",
    "    plt.imshow(train_x[sample_idx, 0].detach().cpu().numpy(), cmap='Reds')\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        pred_y = model(train_x)[sample_idx]\n",
    "    print(\"Predicted: {}, Target: {}\".format(pred_y.detach().cpu().numpy(), train_y[sample_idx].detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.randn(1,2)\n",
    "y1 = torch.randn(1,2)\n",
    "gt = torch.randn(1,2)\n",
    "catX = torch.cat([x1,y1])\n",
    "catY = torch.cat([gt,gt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.L1Loss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_1 = loss(x1,gt)\n",
    "loss_2 = loss(y1,gt)\n",
    "\n",
    "print(loss_1)\n",
    "print(loss_2)\n",
    "\n",
    "# tensor([[0.5266, 0.1171]])\n",
    "# tensor([[0.6186, 0.3922]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_3 = loss(catX,catY)\n",
    "\n",
    "print(loss_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(5)\n",
    "y = np.arange(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx,yy = np.meshgrid(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xx,yy,marker=\".\",color=\"b\",linestyle=\"none\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(3)*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(3)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h,v = np.meshgrid(np.arange(3)*3,np.arange(3)*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id1 = 0\n",
    "id2 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[1,2,3],\n",
    "                [2,5,2],\n",
    "                [1,2,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[id1,id2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h[id1,id2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.array([[0,0,0],\n",
    "       [0,1,0],\n",
    "       [0,0,0],])\n",
    "\n",
    "\n",
    "plt.imshow(img)\n",
    "\n",
    "#y [1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tensor = torch.from_numpy(img)\n",
    "\n",
    "tensor = tensor.unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = CoordConv2d(\"cpu\", 1, 1, kernel_size=1, padding=0, stride=1, input_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1(tensor).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Regression with CoordConv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoordConv2d(nn.Module):\n",
    "    \n",
    "    def __init__(self, device, in_channels, out_channels, kernel_size, padding, stride, input_size):\n",
    "        super(CoordConv2d, self).__init__()\n",
    "        self.device = device\n",
    "        self.cc_xy = self.make_channels(input_size)\n",
    "        self.conv = nn.Conv2d(in_channels+2, out_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "\n",
    "    def make_channels(self, input_size):\n",
    "        coord_vals = (2 * torch.arange(input_size) / input_size) - 1\n",
    "        \n",
    "       \n",
    "        xchannel = coord_vals.repeat((input_size, 1)).unsqueeze(dim=0)\n",
    "        ychannel = xchannel.permute(0, 2, 1)\n",
    "        return torch.cat((xchannel.unsqueeze(dim=0), ychannel.unsqueeze(dim=0)), dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n = x.shape[0]\n",
    "        x = torch.cat((x, self.cc_xy.repeat(n, 1, 1, 1).to(self.device)), dim=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = CoordConv2d(\"cpu\", 3, 3, kernel_size=1, padding=0, stride=1, input_size=64)\n",
    "   \n",
    "            # self.conv1 = nn.Conv2d(1, 1, kernel_size=1, padding=0, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.randn(4,3,64,64).to(device)\n",
    "\n",
    "# conv1(input_tensor).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# models = [CNN(device=device, coordconv=False).to(device), CNN(device=device, coordconv=True).to(device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_Model_1 = BB_model(device,coordConv=True).to(device)\n",
    "\n",
    "regression_Model_2  = BB_model(device,coordConv=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression_Model_1(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tensor = torch.randn(4,3,64,64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_1 = nn.L1Loss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_1(pred_tensor,input_tensor).sum(1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = 0 \n",
    "for j,k in zip(input_tensor,pred_tensor):\n",
    "    \n",
    "    \n",
    "    total_loss += loss_1(k,j).sum(1).sum()\n",
    "    # print(loss_1(k,j).sum(1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "epoch_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainLossModel2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLossModel1 = [68.28050513016551,\n",
    " 65.62488816913806,\n",
    " 59.81700656288549,\n",
    " 47.68163339715255,\n",
    " 23.58207727733411,\n",
    " 9.820071471364875,\n",
    " 7.343100547790527,\n",
    " 6.936299600099263,\n",
    " 6.561544142271343,\n",
    " 6.248961147509124,\n",
    " 5.921486227135909,\n",
    " 5.421173848603901,\n",
    " 4.664885972675524,\n",
    " 3.975954532623291,\n",
    " 3.5830030566767643,\n",
    " 3.2883990312877454,\n",
    " 3.0593762523249577,\n",
    " 2.8676067904422156,\n",
    " 2.7213477084511206,\n",
    " 2.547908155541671,\n",
    " 2.440237747995477,\n",
    " 2.3724605660689506,\n",
    " 2.3558950800644722,\n",
    " 2.3216646345038163,\n",
    " 2.3052869721462854,\n",
    " 2.2401248969529806,\n",
    " 2.3007123846756783,\n",
    " 2.305938858734934,\n",
    " 2.1972941411168954,\n",
    " 2.1649156746111418,\n",
    " 2.104501667775606,\n",
    " 2.1308251995789376,\n",
    " 2.084116032249049,\n",
    " 2.1654670050269678,\n",
    " 2.088952616641396,\n",
    " 2.0328448320689954,\n",
    " 2.036092237422341,\n",
    " 2.0027414372092798,\n",
    " 2.003695845603943,\n",
    " 1.9912696637605365,\n",
    " 1.9280576580449154,\n",
    " 1.9480871087626408,\n",
    " 2.0456484117006,\n",
    " 1.9850581758900692,\n",
    " 1.9068721595563387,\n",
    " 1.8830207837255377,\n",
    " 1.908837356065449,\n",
    " 1.964211476476569,\n",
    " 1.9530157540973865,\n",
    " 1.8911270405116833,\n",
    " 1.8899118461106952,\n",
    " 1.8800873066249646,\n",
    " 1.849120296930012,\n",
    " 1.8649798945376748,\n",
    " 1.8417292707844783,\n",
    " 1.8274180073487132,\n",
    " 1.8325849708757902,\n",
    " 1.8079830533579777,\n",
    " 1.8013163867749666,\n",
    " 1.8247320401041132,\n",
    " 1.80855215223212,\n",
    " 1.7555797100067139,\n",
    " 1.777278605260347,\n",
    " 1.7460961153632717,\n",
    " 1.7326714616072805,\n",
    " 1.7304664348301135,\n",
    " 1.7664734501587718,\n",
    " 1.757043951436093,\n",
    " 1.709508318650095,\n",
    " 1.7285376034284894,\n",
    " 1.6577842172823454,\n",
    " 1.6729020758679038,\n",
    " 1.7605895180451243,\n",
    " 1.7548707347167165,\n",
    " 1.7114581748058921,\n",
    " 1.6845240655698275,\n",
    " 1.7117109235964323,\n",
    " 1.7370394844757884,\n",
    " 1.714644614018892,\n",
    " 1.683782602611341,\n",
    " 1.6904316136711521,\n",
    " 1.650501433171724,\n",
    " 1.635898797135604,\n",
    " 1.6284060415468717,\n",
    " 1.6087490194722225,\n",
    " 1.6075890189722966,\n",
    " 1.601535401846233,\n",
    " 1.5595416332546033,\n",
    " 1.6115675976401882,\n",
    " 1.64955945391404,\n",
    " 1.5978446257741827,\n",
    " 1.5724606827685708,\n",
    " 1.5371948794314736,\n",
    " 1.5596598198539333,\n",
    " 1.5207288767162122,\n",
    " 1.4957340516542132,\n",
    " 1.5806434656444348,\n",
    " 1.5309730391753347,\n",
    " 1.600862415213334,\n",
    " 1.5464605657677901]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valLossModel1 =  [68.90546035766602, 61.56637477874756, 952.7806701660156, 810.3438720703125, 15.786891460418701, 13.454748392105103, 8.070767402648926, 6.924545049667358, 6.517513871192932, 6.228425979614258, 6.068917751312256, 5.536641240119934, 4.160773277282715, 3.6340972781181335, 3.5631832480430603, 3.0644643902778625, 2.78153458237648, 2.654276579618454, 2.45757132768631, 2.413040280342102, 2.308493971824646, 2.2595920860767365, 2.2240691781044006, 2.2414967119693756, 2.1230928897857666, 2.119713634252548, 2.120747923851013, 2.1427029967308044, 2.12529793381691, 2.0213115215301514, 2.0390857458114624, 2.1395826637744904, 1.9609893262386322, 2.0884757936000824, 1.9631588757038116, 1.9820888042449951, 1.914979100227356, 1.9358978867530823, 1.9396913945674896, 1.8884586691856384, 1.9031506776809692, 1.8951497077941895, 1.865460753440857, 1.876004308462143, 1.8659241199493408, 1.885868400335312, 1.8572442531585693, 1.8696589767932892, 1.8276720345020294, 1.9082961976528168, 1.85065758228302, 1.860484391450882, 1.824264407157898, 1.833799421787262, 1.9174040257930756, 1.864008367061615, 1.8715886175632477, 1.8427839279174805, 1.8189807832241058, 1.847440630197525, 1.812896192073822, 1.755580484867096, 1.760489672422409, 1.7367846369743347, 1.8441311419010162, 1.7706548273563385, 1.837672084569931, 1.7540681958198547, 1.7603263556957245, 1.7473979592323303, 1.7059321403503418, 1.7229499816894531, 1.8236200511455536, 1.9254466593265533, 1.7679736614227295, 1.7567171454429626, 1.9713301062583923, 1.7706826031208038, 1.7226713001728058, 1.7720273435115814, 1.7138517796993256, 1.7112639248371124, 1.7340607047080994, 1.6640256643295288, 1.7693849503993988, 1.690043866634369, 1.6531054079532623, 1.6595081388950348, 1.8031052947044373, 1.694405734539032, 1.700481116771698, 1.769815742969513, 1.6867660880088806, 1.6367439329624176, 1.689428985118866, 1.757577896118164, 1.707352727651596, 1.7181978821754456, 1.7547268271446228, 1.691870540380478]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion()\n",
    "\n",
    "plt.plot(epoch_x, valLossModel2, label=\"Standard\", lw=3)\n",
    "plt.plot(epoch_x, valLossModel1, label=\"CoordConv\", lw=3)\n",
    "plt.legend()\n",
    "plt.title(\"Supervised  Validation Regression Error\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LuminEye",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
