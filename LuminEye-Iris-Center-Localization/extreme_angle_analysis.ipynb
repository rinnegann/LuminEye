{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "from basicsr.utils.download_util import load_file_from_url\n",
    "from torchvision import transforms  \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import albumentations as A\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "from imutils import face_utils\n",
    "from scipy.spatial import distance as dist\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe\n",
    "from BaseModels.resnetModels import BB_model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "detector = None\n",
    "predictor = None\n",
    "GAN_MODEL = None\n",
    "IRIS_MODEL = None\n",
    "EYE_AR_THRESH = 0.2\n",
    "\n",
    "mp_face_mesh = mediapipe.solutions.face_mesh\n",
    "\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True)\n",
    "\n",
    "RESIZE_AMT = 64\n",
    "\n",
    "\n",
    "def prediction_image(model,image):\n",
    "        \n",
    "        val_transforms =  A.Compose([\n",
    "                                        A.Resize(width=RESIZE_AMT,height=RESIZE_AMT),\n",
    "                                        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "                                        ToTensorV2(p=1)\n",
    "                                        ])\n",
    "        \n",
    "        unnorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        transformed_img = val_transforms(image=image[:,:,::-1])\n",
    "        image = transformed_img['image']\n",
    "        \n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "                out_coord = model(image)\n",
    "        \n",
    "        \n",
    "        image = image.squeeze(0)\n",
    "\n",
    "        image = transforms.ToPILImage()(unnorm(image))\n",
    "        \n",
    "        \n",
    "        pred_coord = out_coord.detach().cpu().numpy()[0]\n",
    "        \n",
    "        return image,pred_coord\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def captureFaceLandmarks(frame):\n",
    "    \n",
    "    results = face_mesh.process(frame)\n",
    "    landmarks = results.multi_face_landmarks[0]\n",
    "    \n",
    "    shape_arr = []\n",
    "    \n",
    "    for landmark in landmarks.landmark:\n",
    "        \n",
    "        x = landmark.x\n",
    "        y = landmark.y\n",
    "        \n",
    "        relative_x = int(x * frame.shape[1])\n",
    "        relative_y = int(y * frame.shape[0])\n",
    "        \n",
    "        shape_arr.append([relative_x, relative_y])\n",
    "        \n",
    "    \n",
    "    return np.array(shape_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    \"\"\"Load Regression model\n",
    "\n",
    "    Args:\n",
    "        model_path (_str_): _model path_\n",
    "    \n",
    "\n",
    "    Returns:\n",
    "        _torch model_: _RESNET model_\n",
    "    \"\"\"\n",
    "\n",
    "    model = torch.load(model_path,map_location=device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cropped_image(img, shape_array, padded_amt=30):\n",
    "    \"\"\"Cropped eye region \n",
    "\n",
    "    Args:\n",
    "        img (__numpy__): _Original Image_\n",
    "        shape_array (_numpy_): _FaceLandMark locations_\n",
    "        padded_amt (int, optional): _padding size_. Defaults to 15.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Leye = {\"top_left\": shape_array[70], \"bottom_right\": shape_array[133]}\n",
    "\n",
    "    Reye = {\"top_left\": shape_array[285],\n",
    "            \"bottom_right\": shape_array[263]}\n",
    "\n",
    "    left_eye = img[Leye[\"top_left\"][1]:Leye[\"bottom_right\"][1] +\n",
    "                   15\n",
    "                   , Leye[\"top_left\"][0]:Leye[\"bottom_right\"][0]]\n",
    "\n",
    "    right_eye = img[Reye[\"top_left\"][1]:Reye[\"bottom_right\"][1] +\n",
    "                    15, Reye[\"top_left\"][0]:Reye[\"bottom_right\"][0]]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Reye['top_left'][0] = Reye['top_left'][0] - 5\n",
    "\n",
    "    return left_eye, right_eye, Leye, Reye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_coordinate(coord,original_image,resize_amt):\n",
    "    \n",
    "    h,w = original_image.shape[:2]\n",
    "    coord[0] = int((coord[0]/resize_amt) * w)\n",
    "    coord[1] = int((coord[1]/resize_amt) * h)\n",
    "    \n",
    "    return coord\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MediaPipe\n",
    "img_path = \"/home/nipun/Pictures/Webcam/Extreme_Angles/Extreme_LeftAngle.jpg\"\n",
    "frame = cv2.imread(img_path)\n",
    "\n",
    "\n",
    "shape_array = captureFaceLandmarks(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(visualize_iris=True,enhance=True):\n",
    "    \n",
    "    \n",
    "    vid = cv2.VideoCapture(0)\n",
    "    \n",
    "    frameCounter = 0\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        ret, frame = vid.read()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        left_eye, right_eye,Leye,Reye = cropped_image(frame, shape_array)\n",
    "        \n",
    "        \n",
    "\n",
    "            \n",
    "        _,pred_l_eye = prediction_image(model=REGRESSION_MODEL,image=left_eye)\n",
    "        \n",
    "        \n",
    "        _,pred_r_eye = prediction_image(model=REGRESSION_MODEL,image=right_eye)\n",
    "        \n",
    "        \n",
    "        pred_l_eye = rescale_coordinate(pred_l_eye,left_eye,RESIZE_AMT)\n",
    "        \n",
    "        pred_r_eye = rescale_coordinate(pred_r_eye,right_eye,RESIZE_AMT)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        cv2.circle(left_eye,(int(pred_l_eye[0]),int(pred_l_eye[1])),1,(0,255,0),-1)\n",
    "        cv2.circle(right_eye,(int(pred_r_eye[0]),int(pred_r_eye[1])),1,(0,255,0),-1)\n",
    "        \n",
    "        \n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        \n",
    "        \n",
    "        left_eye = cv2.resize(left_eye,(512,512))\n",
    "        right_eye = cv2.resize(right_eye,(512,512))\n",
    "        cv2.imshow(\"Right Eye\",right_eye)\n",
    "        cv2.imshow(\"Left Eye\",left_eye)\n",
    "        \n",
    "\n",
    "        if cv2.waitKey(1) and 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "    vid.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LuminEye",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
